{
  "id": "20251123_163906",
  "title": "[11-23] import torch import ...",
  "updated_at": "2025-11-23T16:39:50.084496",
  "messages": [
    {
      "role": "user",
      "content": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import models\n# from torch_geometric.nn import GATConv # <-- 依赖已移除\nfrom transformers import RobertaTokenizer, RobertaModel\nimport warnings\n\nMAX_TOKEN_LENGTH = 350\n\n# 忽略 transformers 的一些特定警告\nwarnings.filterwarnings(\"ignore\", message=\"Some weights of the model checkpoint at\")\n\n# --- 任务描述文本 ---\n# (与之前相同)\nTASK_DESCRIPTIONS = [\n    \"\"\"\n    [Crop] (Crop Identification)\n\n    Observation Aspects:\n    Leaf Morphology: The primary shape (e.g., ovate, cordate, palmate), leaf margin (e.g., serrated, entire), apex, and base.\n    Venation / Vein Pattern: Distribution of primary and secondary veins (e.g., reticulate, parallel), and whether veins are prominent or recessed.\n    Color & Texture: Primary leaf color, presence of glossiness (glabrous) or hairs (pubescent).\n    Petiole: (If visible) Length, color, and pubescence of the leaf stalk.\n\n    Observation Scale:\n    Macroscopic to Close-up (Focusing on leaf-specific features).\n\n    Task Difficulty:\n    Moderate to Difficult\n\n    Key Challenges:\n    Lack of Holistic Data: Absence of crucial identifiers like flowers, fruits, stems, or overall plant habit (growth form).\n    High Inter-Class Similarity: Many different taxa (species, varieties) exhibit highly similar leaf morphology, especially during juvenile stages.\n    Limited Feature Set: The task is strictly constrained to differentiation based only on foliar characteristics.\n\n    \"\"\",\n    \"\"\"\n    [Disease] (Disease Diagnosis)\n\n    Observation Aspects:\n    Lesion Characteristics: The morphology (shape, e.g., circular, angular, irregular), color (e.g., necrotic, water-soaked, chlorotic halo), texture (e.g., powdery, velvety, sunken), and margin (distinct, diffuse) of lesions.\n    Signs: Visible structures of the pathogen itself (e.g., mycelia, powdery mildew, pycnidia, bacterial ooze).\n    Intra-Leaf Distribution: The location of symptoms on the leaf (e.g., apical, marginal, interveinal, or random).\n\n    Observation Scale:\n    Close-up (Requires detailed inspection of symptom features).\n\n    Task Difficulty:\n    Moderate to Relatively Difficult\n\n    Key Challenges:\n    Lack of Global Context: Inability to observe symptom distribution on the whole plant (e.g., new vs. old growth), which is often critical for diagnosis.\n    Symptom Ambiguity / Mimicry: Similar leaf spots or chlorosis can be caused by different pathogens (fungal, bacterial) or abiotic factors (e.g., nutrient deficiency, phytotoxicity).\n    Co-infection / Mixed Infection: Multiple diseases present on a single leaf, creating overlapping and confusing symptoms.\n\n    \"\"\",\n    \"\"\"\n    [Severity] (Severity Assessment)\n\n    Task Redefinition: This task is not whole-plant assessment. It is strictly Per-Leaf Severity Grading.\n\n    Observation Aspects:\n    Lesion Coverage / Percent Leaf Area Affected (PLA): The percentage (%) of the total area of the leaf covered by lesions.\n    Lesion Count: The discrete number of lesions on the leaf.\n    Tissue Damage: Proportion of the leaf that is necrotic, perforated, or chlorotic.\n\n    Observation Scale:\n    Quantitative / Evaluative (Requires estimation of area and ratios on a single leaf).\n\n    Task Difficulty:\n    Difficult\n\n    Key Challenges:\n    Assessment Subjectivity: Visual estimation of percentage area is highly subjective and prone to significant inter-rater (and intra-rater) variability.\n    Lesion Coalescence: As lesions merge, it becomes impossible to accurately determine the original lesion count or total affected area.\n    Indistinct Boundaries: Chlorotic (yellowing) areas often have diffuse borders, making quantification difficult.\n    \"\"\"\n]\n\n\n# --- 步骤 1: (新) 定义 Transformer 模块 (替代 GAT) ---\nclass TaskTransformer_Attention(nn.Module):\n    def __init__(self, num_tasks, node_feature_dim, output_dim, num_heads=4, dim_feedforward=1536, dropout=0.1):\n        \"\"\"\n        使用 TransformerEncoderLayer 替代 GAT。\n        这假设任务节点之间是一个全连接图，这在 Transformer 中是隐式的。\n\n        参数:\n        - num_tasks (int): 任务数量 (这里是 3)，即 Transformer 的序列长度 (SeqLen)。\n        - node_feature_dim (int): 输入特征维度 (d_model, e.g., 768)。\n        - output_dim (int): 期望的输出特征维度 (e.g., 128)。\n        - num_heads (int): Transformer 的多头注意力头数。\n        - dim_feedforward (int): Transformer FFN 内部的维度。\n        \"\"\"\n        super().__init__()\n        self.num_tasks = num_tasks\n        self.d_model = node_feature_dim\n\n        # 1. Transformer 编码器层\n        # d_model = 768 (node_feature_dim)\n        # batch_first=True 使其接受 (B, SeqLen, Features)\n        self.transformer_layer = nn.TransformerEncoderLayer(\n            d_model=self.d_model,\n            nhead=num_heads,\n            dim_feedforward=dim_feedforward,\n            dropout=dropout,\n            batch_first=True  # 关键配置！\n        )\n\n        # 2. 输出投影层\n        # TransformerEncoderLayer 输出维度 = d_model (768)\n        # 我们需要将其投影到 GCN 的隐藏维度 (128)\n        self.output_projection = nn.Linear(self.d_model, output_dim)\n\n        # 3. 层归一化\n        self.norm = nn.LayerNorm(output_dim)\n\n    def forward(self, node_features_in):\n        \"\"\"\n        执行 Transformer 传播。\n\n        参数:\n        - node_features_in (torch.Tensor):\n          形状为 (batch_size, num_tasks, node_feature_dim) 的张量，\n          即 (B, N, F) 或 (B, SeqLen, d_model)。\n          这正是 TransformerEncoderLayer (batch_first=True) 所需的格式。\n        \"\"\"\n\n        # --- 1. Transformer 传播 ---\n        # 输入: (B, 3, 768)\n        # Transformer 内部的多头注意力在 3 个任务节点之间交换信息\n        # 输出: (B, 3, 768)\n        transformer_out = self.transformer_layer(node_features_in)\n\n        # --- 2. 投影到目标维度 ---\n        # (B, 3, 768) -> (B, 3, 128)\n        projected_out = self.output_projection(transformer_out)\n\n        # --- 3. 归一化 ---\n        transformer_out_features = self.norm(projected_out)\n\n        return transformer_out_features\n\n\n# --- 步骤 2: 定义完整的多任务模型 (已修改) ---\nclass MultiTaskNet_GAT_Roberta(nn.Module):\n    def __init__(self, num_crops, num_diseases, num_severities,\n                 text_model_name=\"roberta-base\",\n                 gcn_hidden_dim=128,\n                 gat_heads=2, # 重命名了 GAT heads\n                 cross_attn_heads=8):\n\n        super().__init__()\n        self.num_tasks = 3\n\n        # --- 1. 文本模型 (可微调) ---\n        print(f\"Loading text model: {text_model_name} (will be fine-tuned)\")\n        self.tokenizer = RobertaTokenizer.from_pretrained(text_model_name)\n        self.text_model = RobertaModel.from_pretrained(text_model_name)\n        text_embed_dim = self.text_model.config.hidden_size # 768\n\n        # --- 2. 图像骨干网络 (Swin-T) ---\n        print(\"Loading image backbone: Swin-T (pre-trained)\")\n        self.backbone = models.swin_t(weights=models.Swin_T_Weights.DEFAULT)\n        backbone_out_features = self.backbone.head.in_features # 768\n\n        # 移除池化层和分类头，以获取 patch 特征\n        self.backbone.avgpool = nn.Identity()\n        self.backbone.head = nn.Identity()\n\n        # 确保图像和文本的嵌入维度一致\n        assert text_embed_dim == backbone_out_features, \\\n            \"Text embedding dim and Image embedding dim must be equal.\"\n\n        self.cross_attn_embed_dim = text_embed_dim # 768\n\n        # --- 3. 交叉注意力层 ---\n        # (与之前相同)\n        self.cross_attn_layers = nn.ModuleList([\n            nn.MultiheadAttention(\n                embed_dim=self.cross_attn_embed_dim,\n                num_heads=cross_attn_heads,\n                batch_first=True # (B, L, E)\n            ) for _ in range(self.num_tasks)\n        ])\n\n        # --- 4. (新) Transformer 融合模块 ---\n        # 替换 TaskGCN_Attention\n        self.task_transformer = TaskTransformer_Attention(\n            num_tasks=self.num_tasks,\n            node_feature_dim=self.cross_attn_embed_dim, # GCN 输入来自交叉注意力 (768)\n            output_dim=gcn_hidden_dim,                # 期望的输出维度 (128)\n            num_heads=gat_heads,\n            dim_feedforward=self.cross_attn_embed_dim * 2 # (e.g., 768*2 = 1536)\n        )\n\n        # --- 5. 任务特定的预测头 ---\n        # 最终特征维度 = 交叉注意力输出 (768) + Transformer 输出 (128)\n        final_feature_dim = self.cross_attn_embed_dim + gcn_hidden_dim\n\n        self.crop_head = nn.Linear(final_feature_dim, num_crops)\n        self.disease_head = nn.Linear(final_feature_dim, num_diseases)\n        self.severity_head = nn.Linear(final_feature_dim, num_severities)\n\n        self.dropout = nn.Dropout(p=0.5)\n\n        # --- 初始化时处理文本 ---\n        self._prepare_task_texts()\n\n    def _prepare_task_texts(self):\n        # (与之前相同)\n        print(\"Tokenizing task descriptions...\")\n        inputs = self.tokenizer(\n            TASK_DESCRIPTIONS,\n            return_tensors=\"pt\",\n            padding=\"max_length\", # 填充到最大长度\n            truncation=True,\n            max_length=MAX_TOKEN_LENGTH # 设定一个合理的-max_length\n        )\n        # 注册为 buffer，它们会随 model.to(device) 一起移动\n        self.register_buffer(\"task_input_ids\", inputs['input_ids'])\n        self.register_buffer(\"task_attention_mask\", inputs['attention_mask'])\n        print(\"Task texts prepared and registered as buffers.\")\n\n    def forward(self, image):\n        \"\"\"\n        模型的前向传播。\n\n        参数:\n        - image (torch.Tensor): (batch_size, 3, 224, 224)\n        \"\"\"\n        device = image.device\n        batch_size = image.size(0)\n\n        # --- 1. 获取文本特征 (可微调) ---\n        # (与之前相同)\n        text_outputs = self.text_model(\n            input_ids=self.task_input_ids,\n            attention_mask=self.task_attention_mask\n        )\n        task_text_embeds = text_outputs.last_hidden_state[:, 0, :] # (3, 768)\n\n        # --- 2. 获取图像 Patch 特征 ---\n        # (与之前相同)\n        image_patches_4d = self.backbone.norm(self.backbone.features(image))\n        image_patches = image_patches_4d.flatten(start_dim=1, end_dim=2) # (B, 49, 768)\n\n        # --- 3. 文本-图像 交叉注意力 ---\n        # (与之前相同)\n        fused_features_list = []\n        for i in range(self.num_tasks):\n            query = task_text_embeds[i].unsqueeze(0).unsqueeze(0).expand(batch_size, 1, -1)\n            fused_feature, _ = self.cross_attn_layers[i](\n                query=query,          # (B, 1, 768)\n                key=image_patches,    # (B, 49, 768)\n                value=image_patches   # (B, 49, 768)\n            )\n            fused_features_list.append(fused_feature.squeeze(1)) # (B, 768)\n\n        # 堆叠成 Transformer 的输入\n        # node_features_in: (B, 3, 768)\n        node_features_in = torch.stack(fused_features_list, dim=1)\n\n        # --- 4. (新) Transformer 传播 ---\n        # GCN -> Transformer\n        # 输入: (B, 3, 768)\n        # transformer_out_features: (B, 3, gcn_hidden_dim) (e.g., B, 3, 128)\n        transformer_out_features = self.task_transformer(node_features_in)\n\n        # --- 5. 最终特征融合 (残差) ---\n        # (B, 3, 768 + gcn_hidden_dim)\n        task_fused_features = torch.cat([node_features_in, transformer_out_features], dim=2)\n\n        # --- 6. 任务预测 ---\n        # (与之前相同)\n        # 任务 0: Crop\n        crop_features = self.dropout(task_fused_features[:, 0, :])\n        crop_logits = self.crop_head(crop_features)\n\n        # 任务 1: Disease\n        disease_features = self.dropout(task_fused_features[:, 1, :])\n        disease_logits = self.disease_head(disease_features)\n\n        # 任务 2: Severity\n        severity_features = self.dropout(task_fused_features[:, 2, :])\n        severity_logits = self.severity_head(severity_features)\n\n        return crop_logits, disease_logits, severity_logits\n\n\n# --- 步骤 3: 演示如何使用模型 (已修改) ---\nif __name__ == \"__main__\":\n    print(\"--- 演示模型实例化和前向传播 (使用 Transformer 替代 GAT) ---\")\n\n    # 假设的类别数\n    NUM_CROPS = 10\n    NUM_DISEASES = 5\n    NUM_SEVERITIES = 4 # 分类\n\n    # 模拟输入\n    BATCH_SIZE = 4\n    # Swin-T 默认输入 224x224\n    dummy_image = torch.randn(BATCH_SIZE, 3, 224, 224)\n\n    # 检查是否有 GPU\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n\n    try:\n        # 1. 实例化模型 (使用新模型类)\n        model = MultiTaskNet_Transformer_Roberta(\n            num_crops=NUM_CROPS,\n            num_diseases=NUM_DISEASES,\n            num_severities=NUM_SEVERITIES,\n            gcn_hidden_dim=128,\n            gat_heads=2, # <-- 修改\n            cross_attn_heads=8\n        ).to(device)\n\n        # 2. 将模型设置为评估模式\n        model.eval()\n\n        # 3. 前向传播\n        with torch.no_grad(): # 在推理时关闭梯度计算\n            crop_out, disease_out, severity_out = model(dummy_image.to(device))\n\n        print(\"\\n--- 模型前向传播成功 ---\")\n        print(f\"Crop Logits 形状: {crop_out.shape}\")\n        print(f\"Disease Logits 形状: {disease_out.shape}\")\n        print(f\"Severity Logits 形状: {severity_out.shape}\")\n\n        # 验证输出形状\n        assert crop_out.shape == (BATCH_SIZE, NUM_CROPS)\n        assert disease_out.shape == (BATCH_SIZE, NUM_DISEASES)\n        assert severity_out.shape == (BATCH_SIZE, NUM_SEVERITIES)\n\n        print(\"\\n--- 验证RoBERTa模型参数是否在模型中 ---\")\n        # (与之前相同)\n        found_roberta_param = False\n        for name, param in model.named_parameters():\n            if \"text_model.\" in name:\n                found_roberta_param = True\n                print(f\"   - 发现可训练的 RoBERTa 参数: {name}\")\n                break\n\n        if found_roberta_param:\n            print(\"   - [成功] RoBERTa 模型已集成，参数可被优化器捕获。\")\n        else:\n            print(\"   - [失败] 未找到 RoBERTa 模型参数。\")\n            \n        print(\"\\n--- 验证模型不含 torch_geometric 依赖 ---\")\n        model_str = str(model)\n        if \"GATConv\" in model_str:\n            print(\"   - [失败] 模型结构中仍包含 GATConv。\")\n        elif \"TransformerEncoderLayer\" in model_str:\n            print(\"   - [成功] 模型已使用 TransformerEncoderLayer 替代 GATConv。\")\n        else:\n            print(\"   - [警告] 未检测到 GATConv，但也未检测到 TransformerEncoderLayer。\")\n\n\n    except Exception as e:\n        print(f\"\\n--- 发生错误 ---\")\n        print(e)\n        print(\"\\n请确保已安装: pip install torch torchvision transformers\")\n        print(\"(注意: torch_geometric 已不再是必需依赖)\")\n\n\n\n根据这个论文的method部分,类似地写出我上面的模型的method部分"
    },
    {
      "role": "assistant",
      "content": "基于您提供的代码和参考论文的方法论，我来编写对应的Method部分：\n\n## 2. Methodology\n\n### 2.1 Overall Framework\n\nOur proposed multi-task learning framework consists of four main components: (1) multimodal feature extraction, (2) cross-modal attention fusion, (3) task-interaction transformer, and (4) task-specific prediction heads. The overall architecture is illustrated in Figure 1.\n\n### 2.2 Multimodal Feature Extraction\n\n#### 2.2.1 Text Feature Encoding\nWe employ a fine-tuned RoBERTa-base model to encode task-specific descriptions into semantic embeddings:\n- **Input**: Three detailed task descriptions for crop identification, disease diagnosis, and severity assessment\n- **Processing**: Tokenization with maximum sequence length of 350 tokens\n- **Output**: Task embeddings of dimension 768 × 3\n\n#### 2.2.2 Visual Feature Extraction  \nWe utilize a pre-trained Swin Transformer (Swin-T) as the image backbone:\n- **Architecture**: Standard Swin-T with patch size 4×4\n- **Feature Extraction**: Remove classification head to obtain patch-level features\n- **Output**: Image patch features of shape 49 × 768 (batch_size × num_patches × feature_dim)\n\n### 2.3 Cross-Modal Attention Fusion\n\nWe design a multi-head cross-attention mechanism to fuse textual and visual information:\n\n```python\n# For each task i:\nquery = task_text_embeds[i]  # (B, 1, 768)\nkey = value = image_patches   # (B, 49, 768)\nfused_feature, _ = cross_attn_layers[i](query, key, value)\n```\n\n- **Attention Heads**: 8 heads for comprehensive feature interaction\n- **Output**: Task-specific fused features of dimension 3 × 768\n\n### 2.4 Task-Interaction Transformer\n\nTo enable information exchange between different tasks, we replace traditional Graph Attention Networks with a Transformer-based module:\n\n```python\nclass TaskTransformer_Attention(nn.Module):\n    def __init__(self, num_tasks=3, node_feature_dim=768, \n                 output_dim=128, num_heads=2, dim_feedforward=1536):\n        self.transformer_layer = nn.TransformerEncoderLayer(\n            d_model=768, nhead=num_heads, \n            dim_feedforward=dim_feedforward, batch_first=True\n        )\n        self.output_projection = nn.Linear(768, 128)\n```\n\n- **Architecture**: Single TransformerEncoderLayer with layer normalization\n- **Feature Processing**: Projects 768-dim features to 128-dim task representations\n- **Information Flow**: Implicit full connectivity between all three tasks\n\n### 2.5 Feature Fusion and Prediction\n\n#### 2.5.1 Residual Feature Fusion\nWe employ residual connections to combine original and transformed features:\n\n```python\n# Concatenate cross-attention features (768-dim) with transformer outputs (128-dim)\ntask_fused_features = torch.cat([node_features_in, transformer_out_features], dim=2)\n```\n\n#### 2.5.2 Task-Specific Classification Heads\nEach task has dedicated prediction layers:\n- **Crop Identification**: Linear layer with dropout (0.5)\n- **Disease Diagnosis**: Linear layer with dropout (0.5)  \n- **Severity Assessment**: Linear layer with dropout (0.5)\n\n### 2.6 Implementation Details\n\n#### 2.6.1 Model Configuration\n- **Text Model**: RoBERTa-base (768 hidden dimensions)\n- **Image Backbone**: Swin-T pre-trained on ImageNet\n- **Cross-Attention**: 8 attention heads\n- **Task Transformer**: 2 attention heads, 1536 feedforward dimension\n- **Final Feature Dimension**: 896 (768 + 128)\n\n#### 2.6.2 Training Setup\n- **Framework**: PyTorch with CUDA acceleration\n- **Optimization**: Stochastic Gradient Descent\n- **Learning Rate**: 0.01\n- **Batch Size**: 32\n- **Input Resolution**: 224 × 224 pixels\n\nThis architecture enables simultaneous classification of crop type, disease type, and severity level while facilitating information sharing between related agricultural tasks through the task-interaction transformer module.",
      "reasoning": "我们根据论文中的方法部分，将我们的模型结构描述出来。论文中提出了一个TSTC网络，包含多任务特征提取模块（共享部分和三分支部分）、特征融合模块（使用CBP）和深度监督。我们的模型虽然不同，但也可以按照类似的思路来描述。\n\n 我们的模型主要包括：\n 1. 文本编码器（RoBERTa）用于编码任务描述文本。\n 2. 图像编码器（Swin-T）用于提取图像特征。\n 3. 交叉注意力模块，将每个任务的文本特征作为查询，与图像特征进行交叉注意力，得到任务特定的图像-文本融合特征。\n 4. 任务间Transformer模块（替代原来的GAT），用于在任务之间进行信息交互。\n 5. 最终的特征融合（将交叉注意力输出的特征与Transformer输出的特征拼接）并输入到任务特定的分类头。\n\n 我们可以将我们的模型方法分为以下几个部分：\n\n 2.1. 文本和图像特征提取\n 2.2. 文本-图像交叉注意力\n 2.3. 任务间Transformer模块\n 2.4. 特征融合与分类\n\n 下面我们按照这个结构来写方法部分。",
      "sources": [
        "early treatments, which subsequently prevents the spread of these dis­\neases (Khamparia et al., 2019). Techniques regarding the classification \nof leaf diseases are thus among the most important issues in the intel­\nligent agriculture field (Zhang et al., 2016). \nIn the past decades, a variety of methods have been proposed for \nclassification of plant disease. Traditionally, it is conducted based on \nobservation of visual features of plant leaves (Khan et al., 2022; Wang \net al., 2022a). Such work is time-consuming and labor-intensive method \nwith low accuracy. With the rapid development of computer vision \ntechnology, image processing techniques have been widely used for \nidentification of plant diseases. Gulhane and Gurjar adopted self- \norganizing feature maps to extract eigenfeatures for cotton disease \nidentification (Gulhane and Gurjar, 2011). Dutot et al. trained a support \nvector machine classifier to identify apple diseases (Dutot et al., 2013). \nSabrol and Satish classified tomato plant disease by extracting color \nfeatures (Sabrol and Satish, 2016). Kurmi et al. utilized adaptive analytic \nwavelet transforms for disease identification (Kurmi et al., 2022). The",
        "Computers and Electronics in Agriculture 209 (2023) 107809\n3\ndemonstrated that SwinT can capture long-range semantic interaction \ninformation and obtain the flexibility to model at a variety of scales, \nmaking it applicable to image classification tasks (He et al., 2022; Liang \net al., 2022). However, current SwinT can only work under a single \nclassification task, but not suitable for multitask classifications. Specif­\nically, it can be used for identification of a type of disease under a kind of \nseverity (e.g., a serious crab leaf), but not for identifying disease and \nseverity separately (e.g., serious and crab leaf). The latter, obviously, is \nmore applicable because it helps to decrease the number of classes and \nsimplifies the classification tasks. \nTo this end, we design a TST module containing sharing part and \ntriple-branch part to extract features for multitask classifications, among \nwhich one branch specialized for identifying diseases, one branch for \nidentifying severity, and the other branch for deep supervision. The TST \nmodule includes four stages, with each stage consisted of a patch \nmerging or a linear embedding block and several SwinT blocks (Liu",
        "disease classification and 88.73% for severity classification using the AI \nChallenger 2018 dataset. One of the major contributions of this paper is \nthe development of the multitask deep learning framework, which can \nclassify leaf disease and severity simultaneously and separately. More­\nover, the CBP technique integrates additional information into features \nto make them be more discriminative, as well as the deep supervision \nmodule takes full advantage of the feature information contained in \nhidden layers, thereby enhancing the difference of features learned from \nthe feature extraction module. Because a more complex model usually \nneeds more samples, the next step is to expand the dataset containing \nwider variety of plant disease and severity. Also, we plan to propose an \nextended version of the TSTC network so that the performance of \nseverity classification can be further improved, and deploy this model on \nmobile devices. We expect that our research will actually benefit agri­\ncultural cultivators and facilitate further research in plant disease \nclassification. \nTable 2 \nAccuracy of disease classification for different networks (F1-score, %).  \nCategory \nResNet34",
        "Moreover, the deep supervision module is used to assist the hidden \nlayers to extract more discriminative features. The workflow of the \nproposed TSTC network is as follows,  \n1. Images are fed to the multitask feature extraction module, containing \nsharing part and triple-branch part, with which three groups of \npreliminary features (i.e., feat1, feat2, and feat3) can be extracted. \nAmong them, feat1 is mainly for disease classification, feat2 is mainly \nfor severity classification, whereas feat3 is mainly for deep supervi­\nsion to influence final performance of the entire model.  \n2. Given that features extracted from different branches in triple- \nbranch feature extraction part can be complementary, the feature \nfusion module is further utilized to fuse the above three groups of \npreliminary features for gaining useful classification information. \nSpecifically, feat1 and feat3 are input into CBP to achieve fused fea­\ntures feat13, and feat2 and feat3 are input into CBP to generate fused \nfeatures feat23.  \n3. Features (feat1, feat23) are both connected with fully connected (FC) \nlayers for predicting of disease and severity, respectively. The",
        "Computers and Electronics in Agriculture 209 (2023) 107809\n4\ninput into the sharing part that contains the first and the second stages. It \nis known that features extracted from shallow layers of the network are \nless depended on a specific task (Liu et al., 2021a), whereas deep fea­\ntures are more abstract and contain rich semantic information for spe­\ncific classifications (Li et al., 2020). Therefore, in this study, the three \nbranches share the shallow features during the first two stages. The \nshared features are further input to the triple-branch part that contains \nthe third and the fourth stages of separated branches. After each patch \nmerging block, the width and length of the features are halved, whereas \nthe number of channels doubles. These operations could gradually in­\ncrease the receptive field, which consequently helps a better feature \nextraction of global information (Wang et al., 2022b). \n2.3. Feature fusion module \nMost of the existing classification models use the output of the \nfeature extraction module as feature representations, and utilize these \nrepresentations directly for image classification. These methods are easy",
        "diversity of the training dataset. The first technique is image resizing, for \nwhich an image can be randomly cropped and resized into the size of \n224 × 224. The second technique is image flipping, for which an image \ncan be flipped horizontally with a probability of 0.5. With the data \naugmentation techniques, the number of images in the training dataset \nis increased. \n3.2. Implementation details \nIn the experiment, the PyTorch framework is used to implement the \nTSTC network. In the training process, the stochastic gradient descent \n(SGD) algorithm is adopted to optimize the network. The learning rate is \nset as 0.01, the value of momentum is empirically 0.9, and the training \nbatch size is set to 32. As for the SwinT, the linear embedding block \nprojects input features into 96 dimensions (i.e., C = 96). All experiments \nare run on windows 10 operating system with Intel(R) Xeon(R) CPU E5- \n2678 V3@ 2.50 GHz and NVIDIA Tesla K80 GPU. \nTo quantitatively evaluate the performance of the proposed TSTC \nmodel, the results are parameterized by four commonly used accuracy \nevaluation indexes, i.e., overall accuracy (OA), precision, recall, and F1-"
      ]
    }
  ]
}