{
  "id": "20251123_130446",
  "title": "[11-23] 文章的标题是什么?",
  "updated_at": "2025-11-23T13:10:16.284109",
  "messages": [
    {
      "role": "user",
      "content": "文章的标题是什么?"
    },
    {
      "role": "assistant",
      "content": "根据参考文档片段，我无法确定文章的完整标题。文档片段中只显示了期刊信息：\n\n- 期刊名称：Computers and Electronics in Agriculture\n- DOI：https://doi.org/10.1016/j.compag.2023.107809\n- 接收日期：2022年9月20日；修订日期：2023年3月13日；接受日期：2023年3月25日\n\n从文档内容可以看出，这篇文章主要研究的是：\n- 植物病害和严重程度的多任务分类\n- 提出了一个基于Swin Transformer的三分支分类网络（TSTC）\n- 同时但分别对病害类型和严重程度进行分类\n\n但具体的文章标题在提供的文档片段中并未完整显示。",
      "reasoning": "",
      "sources": [
        "categories (leaf spot fungus, scorch, no disease, etc.) and 3 severity \ncategories (healthy, general, and serious) with 30,215 images in all. \nFig. 4 illustrates the statistics of the dataset used in this paper. The \nnumber of images for these diseases varies, with only 65 images for \ntarget spot bacteria, whereas with 3,887 images for YLCL virus. The \nnumber of images for the healthy, general and serious severity are \n6,976, 9,494, and 13,745, respectively. For a better representation, \nFig. 5 shows several samples of disease and severity. As can be seen from \nFig. 5, the rearranged dataset focuses on disease type and the corre­\nsponding severity. \nThe dataset is further randomly split into two parts, i.e., training and \nvalidation datasets, with 80% and 20% of these images, respectively. \nMoreover, in order to reduce overfitting problem caused by the limited \nnumber of images and enhance generalization ability of the model, two \nwidely used data augmentation techniques are utilized to enhance and \ndiversity of the training dataset. The first technique is image resizing, for \nwhich an image can be randomly cropped and resized into the size of",
        "the species of plant is usually known for the users or planters (Wang \net al., 2021). Therefore, accurate identification of plant disease and \nseverity is more important, and a new strategy for identifying disease \nand severity is needed, which can improve the versatility and general­\nizability in the task of plant disease identification. However, as far as is \nknow, we are unaware of any work that conducts multitask for identi­\nfying disease and severity simultaneously and separately. \nTo tackle this problem, this paper built a triple-branch SwinT clas­\nsification (TSTC) network to achieve multitask, in order to separate \nclassification of disease and severity for the flexibility of plant disease \nclassification. SwinT can pay attention to the relevant information of \nother adjacent windows, has excellent performance in feature extrac­\ntion. Therefore, we designed a triple-branch SwinT (TST) module based \non SwinT for feature extraction, and then compact bilinear pooling \n(CBP) was utilized to enhance the discrimination ability of extracting \nfeature of disease. Meanwhile, we designed a deep supervision module",
        "features (Sabrol and Satish, 2016). Kurmi et al. utilized adaptive analytic \nwavelet transforms for disease identification (Kurmi et al., 2022). The \n* Corresponding author. \nE-mail address: mkslch2013@163.com (C. Li).  \nContents lists available at ScienceDirect \nComputers and Electronics in Agriculture \njournal homepage: www.elsevier.com/locate/compag \nhttps://doi.org/10.1016/j.compag.2023.107809 \nReceived 20 September 2022; Received in revised form 13 March 2023; Accepted 25 March 2023",
        "disease, and severity are considered as joint categories in most disease classification methods, which will increase \nthe number of categories and decrease the generalization ability of these models. Compared to disease and \nseverity, information on plant species is less important because different species may suffer from the same \ndisease, and such information is usually known to users. Given this, this paper proposed a novel triple-branch \nSwin Transformer classification (TSTC) network for classification of disease and severity simultaneously and \nseparately. The TSTC network consists of a multitask feature extraction module, a feature fusion module and a \ndeep supervision module. Firstly, preliminary features are extracted using a triple-branch network, which is built \nbased on Swin Transformer backbone under the multitask classification strategy (i.e., one for disease classifi­\ncation, one for severity classification and the other for deep supervision). After that, these features are fused \nusing compact bilinear pooling technique to enhance the feature extractor’s learning ability and thus more",
        "models, which joint species-disease (i.e., pepper scab) or species- \ndisease-severity (i.e., strawberry scorch serious) as a category. When \nnew classes are added, the model needs to be retrained, despite crop or \ndisease information has been already existed in the previous dataset (Lee \net al., 2021). To increase the flexibility of plant disease classification \nnetwork, researchers separated species and disease information and \nclassified them for dependently. Wang et al. utilized a dual-stream hi­\nerarchical bilinear pooling model to separately identify species and \ndiseases (Wang et al., 2021). Liang et al. used multitask model PD2SE- \nNet for species identification, disease identification and plant-disease- \nseverity estimation (Liang et al., 2019). In an agricultural environ­\nment, different plants may suffer from the same disease (Prospero and \nCleary, 2017), different severity requires to take corresponding pre­\nventive measures for the same disease (Wang et al., 2017). In addition, \nthe species of plant is usually known for the users or planters (Wang \net al., 2021). Therefore, accurate identification of plant disease and",
        "feature extraction module as feature representations, and utilize these \nrepresentations directly for image classification. These methods are easy \nto use and straightforward. They, however, are not sufficient in the case \nwhen complex diseases occur, and may lead to unsatisfactory classifi­\ncation accuracy (Liang et al., 2019). Studies have shown that fusion of \nfeatures from different branches can help to improve classification \nperformance (Chetouani et al., 2020; Liu and Wu, 2022), which are \nattributed to the complementarity of different features. \nGiven this, a feature fusion module is organized in this paper using a \nwidely used technique, i.e., the CBP technique (Gao et al., 2016). \nDifferent from traditional addition or concatenation of two groups of \nfeatures, CBP first transforms high dimension features from different \nclassification tasks into a low dimensional space using count sketch \nprojection (CSP) function, which can reduce computation and storage \nspace (L´opez-S´anchez et al., 2020). The count sketch projection function \nis adopted because the CSP of the outer product of the input features is"
      ]
    },
    {
      "role": "user",
      "content": "论文的摘要部分给我完整给出"
    },
    {
      "role": "assistant",
      "content": "根据提供的参考文档片段，我无法给出完整的论文摘要。文档片段中主要包含以下内容：\n\n## 可获取的信息片段：\n\n**研究内容概述：**\n- 提出了一种名为TSTC的新型多任务深度学习框架，用于病害和严重程度分类任务\n- 采用SwinT作为主干网络，生成多分支特征提取器\n- 设计了特征融合模块（使用CBP技术）和深度监督模块\n\n**主要贡献：**\n1. 提出了端到端可训练的多任务网络，同时但分别对病害和严重程度进行分类\n2. 提出了TST模块，包含共享部分和三分支部分，用于提取针对特定任务的特征\n3. 设计了深度监督模块，提高隐藏层特征的区分能力\n\n**实验结果：**\n- 在AI Challenger 2018数据集上，病害分类准确率达到99.00%，严重程度分类准确率达到88.73%\n\n**局限性：**\n文档片段不完整，只显示了部分摘要内容，缺少完整的摘要结构和完整的句子表述。\n\n**建议：**\n如需完整的论文摘要，建议查阅原始论文全文或更完整的文档版本。",
      "reasoning": "",
      "sources": [
        "and F1-score were 99.71%, 99.50% and 99.60%, respectively. However, \nthe classification results of serious and general in disease severity are \nlower. From confusion matrix of severity classification (Fig. 9), we find \nFig. 4. Statistics of the dataset used in this paper. The AI Challenger data is rearranged into two categories, i.e., disease type and severity. The numbers in the figure \nrepresent the number of leaves suffering from a given disease type and severity. \nFig. 5. Illustration of the dataset used in the paper.  \nB. Yang et al.",
        "Computers and Electronics in Agriculture 209 (2023) 107809\n11\nCRediT authorship contribution statement \nBin Yang: Conceptualization, Methodology, Writing – review & \nediting, Project administration, Funding acquisition. Zhulian Wang: \nMethodology, Software, Validation, Data curation, Writing – review & \nediting. Jinyuan Guo: Formal analysis, Writing – original draft. Lili \nGuo: Resources, Supervision. Qiaokang Liang: Formal analysis. Qiu \nZeng: Investigation, Supervision. Ruiyuan Zhao: Supervision. Jianwu \nWang: Supervision. Caihong Li: Visualization, Supervision, Project \nadministration. \nDeclaration of Competing Interest \nThe authors declare that they have no known competing financial \ninterests or personal relationships that could have appeared to influence \nthe work reported in this paper. \nData availability \nThe data used in this paper is AI Challenger dataset which can be \npublicly downloaded from https://www.iyiou.com/t/AI_Challenger_ \nquanqiuAItiaozhansai. \nAcknowledgement \nThis work was supported by 2022 Cotton Science and Technology \nInnovation Project of Hunan Province under Grant XCJZ[2022]-51, the \nNational Natural Science Foundation of China under Grant 41801227,",
        "l3, l4 and l5 integrated are increased by 1.16% and 1.75% in disease and \nseverity classification, respectively. The great improvement of the TSTC \nnetwork demonstrates the effectiveness of deep supervision module. \n5. Conclusion \nIn this paper, a novel multitask deep learning framework called TSTC \nwas proposed for disease and severity classification tasks. It adopted \nSwinT as backbone for generating a multi-branch feature extractor, \nwhich can not only capture long-range semantic interaction information \nbut can extract features targeted for specific tasks. To make features be \nmore discriminative, a feature fusion module was designed using CBP \ntechnique to fuse cross-branch features. In addition, a deep supervision \nmodule was also proposed to supervise hidden layers to generate more \ndiscriminative features in hidden layers. Compared with five widely \nused classification networks (i.e., VGG16, VGG19, ResNet34, ResNet50, \nand SwinT), our TSTC network achieves best accuracy with 99.00% for \ndisease classification and 88.73% for severity classification using the AI \nChallenger 2018 dataset. One of the major contributions of this paper is",
        "model, the results are parameterized by four commonly used accuracy \nevaluation indexes, i.e., overall accuracy (OA), precision, recall, and F1- \nscore (Liu et al., 2022). The definitions of OA, precision, recall, and F1- \nscore are shown in Eq. (9–12). In this paper, OA is an overall indicator \nthat used to quantitatively evaluate the overall performance of the \nnetwork. Precision, recall, and F1-score are used to evaluate accuracy of \neach category (i.e., disease type or severity). In addition, confusion \nmatrix is used to further validate the performance of the model for each \ncategory. \nOA =\nTP + TN\nTP + FP + TN + FN\n(9)  \nPrecision =\nTP\nTP + FP\n(10)  \nRecall =\nTP\nTP + FN\n(11)  \nF1 −score = 2*precision*recall\nprecision + recall\n(12)  \nwhere TP is True Positive, TN is True Negative, FP is False Positive, and \nB. Yang et al.",
        "feature extraction module as feature representations, and utilize these \nrepresentations directly for image classification. These methods are easy \nto use and straightforward. They, however, are not sufficient in the case \nwhen complex diseases occur, and may lead to unsatisfactory classifi­\ncation accuracy (Liang et al., 2019). Studies have shown that fusion of \nfeatures from different branches can help to improve classification \nperformance (Chetouani et al., 2020; Liu and Wu, 2022), which are \nattributed to the complementarity of different features. \nGiven this, a feature fusion module is organized in this paper using a \nwidely used technique, i.e., the CBP technique (Gao et al., 2016). \nDifferent from traditional addition or concatenation of two groups of \nfeatures, CBP first transforms high dimension features from different \nclassification tasks into a low dimensional space using count sketch \nprojection (CSP) function, which can reduce computation and storage \nspace (L´opez-S´anchez et al., 2020). The count sketch projection function \nis adopted because the CSP of the outer product of the input features is",
        "(CBP) was utilized to enhance the discrimination ability of extracting \nfeature of disease. Meanwhile, we designed a deep supervision module \nto enhance the capability of classification. To summarize, the contri­\nbutions of this paper are, \n1) We proposed an end-to-end trainable multitask network for classi­\nfication of disease and severity simultaneously and separately to \nimprove the network’s generalization ability and the application in \nagriculture field.  \n2) We proposed a TST module, which contained sharing part and triple- \nbranch part, to extract features targeted for special task. And \nattributed to the complementarity of different features, we adopted \nCBP for further fusing cross-branch feature information to make \nfeatures more distinctive.  \n3) We explored methods for improving feature extraction ability, then \ndeep supervision module was designed to generate more distinctive \nfeaturesof hidden layers to improve the classification accuracy of \ndisease and severity. \nThe rest of this paper is organized as follows: Section 2 introduces the \nproposed network in detail. Then, Section 3 presents dataset, the results"
      ]
    },
    {
      "role": "user",
      "content": "conclusion部分讲了什么"
    },
    {
      "role": "assistant",
      "content": "根据提供的参考文档片段，虽然没有明确标为“Conclusion”的部分，但可以从文档的结尾部分提取出该研究的核心结论和贡献总结。以下是基于文档内容的结论部分要点：\n\n## 研究结论与贡献总结\n\n### 1. 提出的方法\n- 提出了一个名为 **TSTC** 的端到端可训练多任务网络\n- 能够**同时但分别**对植物**病害类型**和**严重程度**进行分类\n\n### 2. 核心创新点\n**任务特异性变换模块**\n- 包含共享部分和三分支部分\n- 针对不同任务提取特异性特征\n- 通过跨分支特征融合增强特征区分能力\n\n**深度监督模块**\n- 增强隐藏层的特征提取能力\n- 提高整体分类准确率\n\n### 3. 性能表现\n**病害分类**\n- 对大多数病害类别获得最高F1分数\n- 白粉病和黑麻疹真菌的F1分数达到100%\n\n**严重程度分类**\n- 健康叶片：F1 = 99.60%\n- 一般严重：F1 = 81.95%  \n- 严重病害：F1 = 87.80%\n- 相比SwinT网络分别提升1.32%、3.23%和2.46%\n\n### 4. 方法有效性验证\n通过可视化分析和消融实验证明：\n- 特征提取模块能够有效聚焦于病变区域\n- 特征融合策略显著提升模型性能\n- 深度监督机制有效改善特征提取能力\n\n### 5. 总体评价\n该研究提出的TSTC网络在植物病害分类任务中表现出优越性能，为农业领域的智能病害诊断提供了有效的解决方案。",
      "reasoning": "好的，用户现在问的是conclusion部分讲了什么。首先，我需要回顾之前的对话历史。用户之前问了标题和摘要，但文档片段里没有完整信息，现在转向结论部分。\n\n根据提供的参考文档，虽然没直接标出“Conclusion”，但最后几段应该包含总结性的内容。比如文档里提到了“To summarize, the contributions...”以及实验结果的总结，这些可能属于结论部分。\n\n用户可能是研究者或者学生，正在快速获取论文的核心结论，以便评估是否精读全文。深层需求可能是想了解论文的创新点和实际应用价值，而不仅仅是表面内容。\n\n需要从文档中提取关键点：主要贡献、方法创新、实验结果和未来方向。注意用户之前提到摘要不完整，所以结论部分要尽量整合分散的信息，确保全面。\n\n另外，用户可能希望用清晰的中文和Markdown格式，所以回答要结构分明，突出要点，避免冗长。还要检查是否有遗漏的重要结论，比如模型性能提升的具体数据或应用前景。",
      "sources": [
        "cation are shown in Table 2 and Table 3. In disease classification, F1- \nscore of most of the disease categories of our TSTC model is the \nhighest. Powdery mildew and black measles fungus even obtain the F1- \nscore of 100%. In severity classification, our TSTC network achieves the \nhighest F1 of 99.60%, 81.95% and 87.80% for healthy, general and \nserious, respectively, which are 1.32%, 3.23% and 2.46% higher than \nthose obtained by SwinT. These results demonstrates that our proposed \nTSTC network provides the best performance among these widely used \nclassification networks. \n4. Discussions \n4.1. Visualization of deep features \nIn order to further explore the feature extraction ability of our pro­\nposed TSTC network, Gradient-weighted Class Activation Map (Selvar­\naju et al., 2017) is used to visualize the features from last layer of the \nfeature extraction module (i.e., feat1 and feat2), as shown in the heat map \nof Fig. 10. It is notable that the deeper the red color, the more attention \nare paid to the features of this part (Liu et al., 2021c). Comparison be­\ntween the disease images and the corresponding heat map suggests the",
        "(CBP) was utilized to enhance the discrimination ability of extracting \nfeature of disease. Meanwhile, we designed a deep supervision module \nto enhance the capability of classification. To summarize, the contri­\nbutions of this paper are, \n1) We proposed an end-to-end trainable multitask network for classi­\nfication of disease and severity simultaneously and separately to \nimprove the network’s generalization ability and the application in \nagriculture field.  \n2) We proposed a TST module, which contained sharing part and triple- \nbranch part, to extract features targeted for special task. And \nattributed to the complementarity of different features, we adopted \nCBP for further fusing cross-branch feature information to make \nfeatures more distinctive.  \n3) We explored methods for improving feature extraction ability, then \ndeep supervision module was designed to generate more distinctive \nfeaturesof hidden layers to improve the classification accuracy of \ndisease and severity. \nThe rest of this paper is organized as follows: Section 2 introduces the \nproposed network in detail. Then, Section 3 presents dataset, the results",
        "Computers and Electronics in Agriculture 209 (2023) 107809\n6\nFN is False Negative. True Positive denotes the number of positive \nsamples correctly categorized as positive samples. True Negative de­\nnotes the number of negative samples correctly categorized as negative \nsamples. False Positive denotes the number of negative samples incor­\nrectly categorized as positive samples. False Negative denotes the \nnumber of positive samples incorrectly categorized as negative samples. \n3.3. Results of disease classification \nHere, the performance of the proposed TSTC network for disease \nclassification is presented, as shown in Fig. 6. It can be seen that the \nthree evaluation indexes for all disease categories reach 96% or more \nexcept for three categories, for which the evaluation indexes vary be­\ntween 84.00% and 95.00%. The reason is that the plant disease samples \nare seriously imbalanced, and the network do not learn enough infor­\nmation about the features of this category, e.g., Target Spot Bacteria \n(only 65 images for this disease) compared to other diseases, and thus \nleads to a relatively low accuracy. As shown in Fig. 7, the confusion",
        "Moreover, the deep supervision module is used to assist the hidden \nlayers to extract more discriminative features. The workflow of the \nproposed TSTC network is as follows,  \n1. Images are fed to the multitask feature extraction module, containing \nsharing part and triple-branch part, with which three groups of \npreliminary features (i.e., feat1, feat2, and feat3) can be extracted. \nAmong them, feat1 is mainly for disease classification, feat2 is mainly \nfor severity classification, whereas feat3 is mainly for deep supervi­\nsion to influence final performance of the entire model.  \n2. Given that features extracted from different branches in triple- \nbranch feature extraction part can be complementary, the feature \nfusion module is further utilized to fuse the above three groups of \npreliminary features for gaining useful classification information. \nSpecifically, feat1 and feat3 are input into CBP to achieve fused fea­\ntures feat13, and feat2 and feat3 are input into CBP to generate fused \nfeatures feat23.  \n3. Features (feat1, feat23) are both connected with fully connected (FC) \nlayers for predicting of disease and severity, respectively. The",
        "are paid to the features of this part (Liu et al., 2021c). Comparison be­\ntween the disease images and the corresponding heat map suggests the \ndiseased areas are basically consistent with the red parts. The extracted \ndeep feature for severity classification pays extensive attention to the \ndiseased part. As for disease classification, besides the diseased part, the \nextracted deep features also focus on regions surrounding the diseased \nareas. In addition, for the four healthy leaves in Fig, 7 (a), (d), (g), and \n(j), the features for two classification tasks are almost focused on the \nsame part, i.e., the whole leaf. It is thus clear that discriminative features \ncan be extracted via the feature extraction module. \n4.2. Effectiveness of feature fusion \nTo explore the effectiveness of the feature fusion module, we per­\nformed ablation experiments using several feature fusion strategies, i.e., \nthe TSTC network without using CBP as well as using CBP to fuse \nFig. 6. Disease classification results of the proposed TSTC network.  \nB. Yang et al.",
        "the OA of disease and severity classification is 97.84% and 86.98%, \nrespectively. Incorporated l3 or l4 or l5 into the training process can \nimprove the network performance. For example, the OA can be \nimproved by 0.12% and 0.8% after adding l3 to final loss. It indicates \nthat deep supervision of hidden layers can improve the feature extrac­\ntion ability and thus improves the accuracy of the model. When any two \nlosses of the deep supervision module are used, the OA of both classi­\nfication tasks is improved compared to that without deep supervision. \nHowever, the case is different compared with using only one of the three \nlosses. Take “Base+l3 + l5” model as an example, the accuracy of \nseverity classification is 0.39% higher than “Base+l5” model, whereas \nthe accuracy of disease classification is higher than “Base + l3” model \nand the accuracy of severity classification is lower than “Base + l3” \nmodel. Notably, compared to the “Base” model, the OA of the TSTC with \nl3, l4 and l5 integrated are increased by 1.16% and 1.75% in disease and \nseverity classification, respectively. The great improvement of the TSTC"
      ]
    }
  ]
}