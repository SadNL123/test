{
  "id": "20251123_131934",
  "title": "[11-23] 文章标题是什么",
  "updated_at": "2025-11-23T13:30:51.810612",
  "messages": [
    {
      "role": "user",
      "content": "文章标题是什么"
    },
    {
      "role": "assistant",
      "content": "根据提供的参考文档，文章标题是：\n\n**Identifying plant disease and severity from leaves: A deep multitask learning framework using triple-branch Swin Transformer and deep supervision**\n\n（中文翻译：从叶片识别植物病害及其严重程度：一种使用三分支 Swin Transformer 和深度监督的深度多任务学习框架）",
      "reasoning": "",
      "sources": [
        "categories (leaf spot fungus, scorch, no disease, etc.) and 3 severity \ncategories (healthy, general, and serious) with 30,215 images in all. \nFig. 4 illustrates the statistics of the dataset used in this paper. The \nnumber of images for these diseases varies, with only 65 images for \ntarget spot bacteria, whereas with 3,887 images for YLCL virus. The \nnumber of images for the healthy, general and serious severity are \n6,976, 9,494, and 13,745, respectively. For a better representation, \nFig. 5 shows several samples of disease and severity. As can be seen from \nFig. 5, the rearranged dataset focuses on disease type and the corre­\nsponding severity. \nThe dataset is further randomly split into two parts, i.e., training and \nvalidation datasets, with 80% and 20% of these images, respectively. \nMoreover, in order to reduce overfitting problem caused by the limited \nnumber of images and enhance generalization ability of the model, two \nwidely used data augmentation techniques are utilized to enhance and \ndiversity of the training dataset. The first technique is image resizing, for \nwhich an image can be randomly cropped and resized into the size of",
        "features (Sabrol and Satish, 2016). Kurmi et al. utilized adaptive analytic \nwavelet transforms for disease identification (Kurmi et al., 2022). The \n* Corresponding author. \nE-mail address: mkslch2013@163.com (C. Li).  \nContents lists available at ScienceDirect \nComputers and Electronics in Agriculture \njournal homepage: www.elsevier.com/locate/compag \nhttps://doi.org/10.1016/j.compag.2023.107809 \nReceived 20 September 2022; Received in revised form 13 March 2023; Accepted 25 March 2023",
        "feature extraction module as feature representations, and utilize these \nrepresentations directly for image classification. These methods are easy \nto use and straightforward. They, however, are not sufficient in the case \nwhen complex diseases occur, and may lead to unsatisfactory classifi­\ncation accuracy (Liang et al., 2019). Studies have shown that fusion of \nfeatures from different branches can help to improve classification \nperformance (Chetouani et al., 2020; Liu and Wu, 2022), which are \nattributed to the complementarity of different features. \nGiven this, a feature fusion module is organized in this paper using a \nwidely used technique, i.e., the CBP technique (Gao et al., 2016). \nDifferent from traditional addition or concatenation of two groups of \nfeatures, CBP first transforms high dimension features from different \nclassification tasks into a low dimensional space using count sketch \nprojection (CSP) function, which can reduce computation and storage \nspace (L´opez-S´anchez et al., 2020). The count sketch projection function \nis adopted because the CSP of the outer product of the input features is",
        "field of plant diseases and severity classification (Gao et al., 2021). It \nconsists of 31,718 disease images, which are divided into 61 categories \nby species, disease type or severity, such as potato early bling fungus \ngeneral, and tomato early bling fungus general, et al. It is notable that \nthis paper aims to identify plant disease and the corresponding severity \naccurately, the leaf species information is thus less important because \ndifferent species may suffer from the same disease and considering more \ncategories will decrease the generalization ability of the proposed \nmodel. Given this, this paper ignores plant species category and rear­\nrange the AI Challenger 2018 dataset into two categories, i.e., disease \nand severity. All healthy leaves, despite from different species, are \nclassified to the “no disease” type. Three diseases without severity in­\nformation (i.e., apple frogeye spot, corn maize dwarf mosaic virus, to­\nmato TOMV) in the dataset are excluded, remaining 20 disease \ncategories (leaf spot fungus, scorch, no disease, etc.) and 3 severity \ncategories (healthy, general, and serious) with 30,215 images in all.",
        "the species of plant is usually known for the users or planters (Wang \net al., 2021). Therefore, accurate identification of plant disease and \nseverity is more important, and a new strategy for identifying disease \nand severity is needed, which can improve the versatility and general­\nizability in the task of plant disease identification. However, as far as is \nknow, we are unaware of any work that conducts multitask for identi­\nfying disease and severity simultaneously and separately. \nTo tackle this problem, this paper built a triple-branch SwinT clas­\nsification (TSTC) network to achieve multitask, in order to separate \nclassification of disease and severity for the flexibility of plant disease \nclassification. SwinT can pay attention to the relevant information of \nother adjacent windows, has excellent performance in feature extrac­\ntion. Therefore, we designed a triple-branch SwinT (TST) module based \non SwinT for feature extraction, and then compact bilinear pooling \n(CBP) was utilized to enhance the discrimination ability of extracting \nfeature of disease. Meanwhile, we designed a deep supervision module",
        "early treatments, which subsequently prevents the spread of these dis­\neases (Khamparia et al., 2019). Techniques regarding the classification \nof leaf diseases are thus among the most important issues in the intel­\nligent agriculture field (Zhang et al., 2016). \nIn the past decades, a variety of methods have been proposed for \nclassification of plant disease. Traditionally, it is conducted based on \nobservation of visual features of plant leaves (Khan et al., 2022; Wang \net al., 2022a). Such work is time-consuming and labor-intensive method \nwith low accuracy. With the rapid development of computer vision \ntechnology, image processing techniques have been widely used for \nidentification of plant diseases. Gulhane and Gurjar adopted self- \norganizing feature maps to extract eigenfeatures for cotton disease \nidentification (Gulhane and Gurjar, 2011). Dutot et al. trained a support \nvector machine classifier to identify apple diseases (Dutot et al., 2013). \nSabrol and Satish classified tomato plant disease by extracting color \nfeatures (Sabrol and Satish, 2016). Kurmi et al. utilized adaptive analytic \nwavelet transforms for disease identification (Kurmi et al., 2022). The"
      ]
    },
    {
      "role": "user",
      "content": "先回答论文的标题,然后总结摘要和discussion部分的内容"
    },
    {
      "role": "assistant",
      "content": "根据提供的参考文档，以下是关于该论文的信息：\n\n## 论文标题\n**Identifying plant disease and severity from leaves: A deep multitask learning framework using triple-branch Swin Transformer and deep supervision**\n\n## 摘要部分总结\n\n该论文提出了一种名为TSTC的新型多任务深度学习框架，用于同时进行植物病害分类和严重程度分类任务。该框架的主要特点包括：\n\n- **核心架构**：采用Swin Transformer（SwinT）作为主干网络，构建多分支特征提取器\n- **技术优势**：\n  - 能够捕获长距离语义交互信息\n  - 提取针对特定任务的特征\n- **创新模块**：\n  - 特征融合模块：使用CBP技术融合跨分支特征，使特征更具判别性\n  - 深度监督模块：监督隐藏层生成更具判别性的特征\n- **性能表现**：在AI Challenger 2018数据集上，与VGG16、VGG19、ResNet34、ResNet50和SwinT等主流分类网络相比，TSTC网络取得了最佳准确率，病害分类准确率达99.00%，严重程度分类准确率达88.73%\n\n## Discussion部分总结\n\n**有效性验证**：\n- 通过消融实验验证了特征融合模块的有效性\n- 深度监督模块显著提升了模型性能：当不使用深度监督时，病害和严重程度分类的OA分别为97.84%和86.98%；集成l3、l4和l5三个损失后，两个分类任务的OA分别提高了1.16%和1.75%\n\n**主要贡献**：\n- 开发了能够同时且分别分类叶片病害和严重程度的多任务深度学习框架\n- CBP技术整合额外信息到特征中，使其更具判别性\n- 深度监督模块充分利用隐藏层中的特征信息\n\n**未来工作方向**：\n- 扩展包含更广泛植物病害和严重程度的数据集\n- 提出TSTC网络的扩展版本，进一步提升严重程度分类性能\n- 将该模型部署到移动设备上，期望研究成果能够实际惠及农业种植者并促进植物病害研究的进一步发展",
      "reasoning": "",
      "sources": [
        "diseased part. As for disease classification, besides the diseased part, the \nextracted deep features also focus on regions surrounding the diseased \nareas. In addition, for the four healthy leaves in Fig, 7 (a), (d), (g), and \n(j), the features for two classification tasks are almost focused on the \nsame part, i.e., the whole leaf. It is thus clear that discriminative features \ncan be extracted via the feature extraction module. \n4.2. Effectiveness of feature fusion \nTo explore the effectiveness of the feature fusion module, we per­\nformed ablation experiments using several feature fusion strategies, i.e., \nthe TSTC network without using CBP as well as using CBP to fuse \nFig. 6. Disease classification results of the proposed TSTC network.  \nB. Yang et al.",
        "gration any two losses in deep supervision module. Where k, h∈{3,4,5, \nand k ∕= h. The TSTC network is our proposed framework which com­\nbines three losses of deep supervision module and final loss jointly to \noptimize the model. \nAs can be seen from Table 5, when the deep supervision module is \nnot adopted to the model, the accuracy of the network is the lowest, and \nthe OA of disease and severity classification is 97.84% and 86.98%, \nrespectively. Incorporated l3 or l4 or l5 into the training process can \nimprove the network performance. For example, the OA can be \nimproved by 0.12% and 0.8% after adding l3 to final loss. It indicates \nthat deep supervision of hidden layers can improve the feature extrac­\ntion ability and thus improves the accuracy of the model. When any two \nlosses of the deep supervision module are used, the OA of both classi­\nfication tasks is improved compared to that without deep supervision.",
        "losses of the deep supervision module are used, the OA of both classi­\nfication tasks is improved compared to that without deep supervision. \nHowever, the case is different compared with using only one of the three \nlosses. Take “Base+l3 + l5” model as an example, the accuracy of \nseverity classification is 0.39% higher than “Base+l5” model, whereas \nthe accuracy of disease classification is higher than “Base + l3” model \nand the accuracy of severity classification is lower than “Base + l3” \nmodel. Notably, compared to the “Base” model, the OA of the TSTC with \nl3, l4 and l5 integrated are increased by 1.16% and 1.75% in disease and \nseverity classification, respectively. The great improvement of the TSTC \nnetwork demonstrates the effectiveness of deep supervision module. \n5. Conclusion \nIn this paper, a novel multitask deep learning framework called TSTC \nwas proposed for disease and severity classification tasks. It adopted",
        "Computers and Electronics in Agriculture 209 (2023) 107809\n11\nCRediT authorship contribution statement \nBin Yang: Conceptualization, Methodology, Writing – review & \nediting, Project administration, Funding acquisition. Zhulian Wang: \nMethodology, Software, Validation, Data curation, Writing – review & \nediting. Jinyuan Guo: Formal analysis, Writing – original draft. Lili \nGuo: Resources, Supervision. Qiaokang Liang: Formal analysis. Qiu \nZeng: Investigation, Supervision. Ruiyuan Zhao: Supervision. Jianwu \nWang: Supervision. Caihong Li: Visualization, Supervision, Project \nadministration. \nDeclaration of Competing Interest \nThe authors declare that they have no known competing financial \ninterests or personal relationships that could have appeared to influence \nthe work reported in this paper. \nData availability \nThe data used in this paper is AI Challenger dataset which can be \npublicly downloaded from https://www.iyiou.com/t/AI_Challenger_ \nquanqiuAItiaozhansai. \nAcknowledgement",
        "Challenger 2018 dataset. One of the major contributions of this paper is \nthe development of the multitask deep learning framework, which can \nclassify leaf disease and severity simultaneously and separately. More­\nover, the CBP technique integrates additional information into features \nto make them be more discriminative, as well as the deep supervision \nmodule takes full advantage of the feature information contained in \nhidden layers, thereby enhancing the difference of features learned from \nthe feature extraction module. Because a more complex model usually \nneeds more samples, the next step is to expand the dataset containing \nwider variety of plant disease and severity. Also, we plan to propose an \nextended version of the TSTC network so that the performance of \nseverity classification can be further improved, and deploy this model on \nmobile devices. We expect that our research will actually benefit agri­\ncultural cultivators and facilitate further research in plant disease",
        "In this paper, a novel multitask deep learning framework called TSTC \nwas proposed for disease and severity classification tasks. It adopted \nSwinT as backbone for generating a multi-branch feature extractor, \nwhich can not only capture long-range semantic interaction information \nbut can extract features targeted for specific tasks. To make features be \nmore discriminative, a feature fusion module was designed using CBP \ntechnique to fuse cross-branch features. In addition, a deep supervision \nmodule was also proposed to supervise hidden layers to generate more \ndiscriminative features in hidden layers. Compared with five widely \nused classification networks (i.e., VGG16, VGG19, ResNet34, ResNet50, \nand SwinT), our TSTC network achieves best accuracy with 99.00% for \ndisease classification and 88.73% for severity classification using the AI \nChallenger 2018 dataset. One of the major contributions of this paper is \nthe development of the multitask deep learning framework, which can"
      ]
    },
    {
      "role": "user",
      "content": "按照原文的层级组织方式给我介绍discussion部分"
    },
    {
      "role": "assistant",
      "content": "好的，遵照要求，按照原文的层级组织方式介绍 **Discussion（讨论）** 部分。\n\n---\n\n### **4. 讨论**\n\n本文的讨论部分通过系统的消融实验和可视化分析，验证了所提出TSTC网络各个核心模块的有效性。\n\n#### **4.1 深度特征的可视化**\n\n为了探究TSTC网络的特征提取能力，该研究使用了**梯度加权类激活图** 来可视化特征提取模块最后一层的特征。\n\n*   **观察结果**：\n    *   对于**严重程度分类**任务，提取的深度特征主要关注**病变区域本身**。\n    *   对于**病害分类**任务，提取的深度特征除了关注病变区域外，还会关注**病变区域周围的区域**。\n    *   对于健康的叶片，两个分类任务所关注的特征区域基本一致，即**整个叶片**。\n*   **结论**：该可视化结果表明，TSTC网络的特征提取模块能够成功提取出具有判别性的特征。\n\n#### **4.2 特征融合的有效性**\n\n为了探索特征融合模块的有效性，研究团队进行了消融实验，比较了多种特征融合策略。\n\n*   **实验设置**：比较了TSTC网络在不使用CBP、使用CBP融合不同分支组合等情况下的性能。\n*   **关键发现**：\n    *   在TSTC网络中嵌入**两个CBP模块**（CBP2和CBP3）能取得最佳性能。\n    *   使用**三个CBP模块**进行融合反而会导致准确率下降，推测原因是**特征冗余**给模型带来了负担。\n*   **最终选择**：基于以上结果，本文最终选择使用CBP2和CBP3来进行实验。\n\n#### **4.3 深度监督的有效性**\n\n为了研究深度监督模块对多任务分类网络的影响，本文对是否使用该模块的损失（l3, l4, l5）进行了消融实验。\n\n*   **实验设置**：\n    *   **Base (w/o l3 + l4 + l5)**：不使用深度监督模块的基础模型。\n    *   **Base + lk**：在基础模型上引入一个深度监督损失（k∈{3,4,5}）。\n    *   **Base + lk + lh**：在基础模型上引入任意两个深度监督损失（k, h∈{3,4,5}, 且 k≠h）。\n    *   **TSTC**：完整框架，同时整合l3, l4, l5三个损失和最终损失共同优化模型。\n*   **实验结果与分析**：\n    *   不采用深度监督模块时，网络的准确率最低（病害分类OA: 97.84%，严重程度分类OA: 86.98%）。\n    *   在训练过程中加入l3、l4或l5中的任何一个，都能提升网络性能。例如，加入l3后，准确率分别提升了0.12%和0.8%。这表明对隐藏层进行深度监督可以**提升特征提取能力**，从而提高模型准确率。\n    *   当使用深度监督模块中的任意两个损失时，两个分类任务的准确率相较于无监督的情况均有提升，但与仅使用一个损失的情况相比，结果各有优劣。\n    *   **最重要的发现**：与\"Base\"模型相比，完整集成了l3、l4和l5的TSTC网络，在病害和严重程度分类上的OA分别**显著提高了1.16%和1.75%**。这一巨大改进充分证明了**深度监督模块的有效性**。",
      "reasoning": "",
      "sources": [
        "gration any two losses in deep supervision module. Where k, h∈{3,4,5, \nand k ∕= h. The TSTC network is our proposed framework which com­\nbines three losses of deep supervision module and final loss jointly to \noptimize the model. \nAs can be seen from Table 5, when the deep supervision module is \nnot adopted to the model, the accuracy of the network is the lowest, and \nthe OA of disease and severity classification is 97.84% and 86.98%, \nrespectively. Incorporated l3 or l4 or l5 into the training process can \nimprove the network performance. For example, the OA can be \nimproved by 0.12% and 0.8% after adding l3 to final loss. It indicates \nthat deep supervision of hidden layers can improve the feature extrac­\ntion ability and thus improves the accuracy of the model. When any two \nlosses of the deep supervision module are used, the OA of both classi­\nfication tasks is improved compared to that without deep supervision.",
        "losses of the deep supervision module are used, the OA of both classi­\nfication tasks is improved compared to that without deep supervision. \nHowever, the case is different compared with using only one of the three \nlosses. Take “Base+l3 + l5” model as an example, the accuracy of \nseverity classification is 0.39% higher than “Base+l5” model, whereas \nthe accuracy of disease classification is higher than “Base + l3” model \nand the accuracy of severity classification is lower than “Base + l3” \nmodel. Notably, compared to the “Base” model, the OA of the TSTC with \nl3, l4 and l5 integrated are increased by 1.16% and 1.75% in disease and \nseverity classification, respectively. The great improvement of the TSTC \nnetwork demonstrates the effectiveness of deep supervision module. \n5. Conclusion \nIn this paper, a novel multitask deep learning framework called TSTC \nwas proposed for disease and severity classification tasks. It adopted",
        "diseased part. As for disease classification, besides the diseased part, the \nextracted deep features also focus on regions surrounding the diseased \nareas. In addition, for the four healthy leaves in Fig, 7 (a), (d), (g), and \n(j), the features for two classification tasks are almost focused on the \nsame part, i.e., the whole leaf. It is thus clear that discriminative features \ncan be extracted via the feature extraction module. \n4.2. Effectiveness of feature fusion \nTo explore the effectiveness of the feature fusion module, we per­\nformed ablation experiments using several feature fusion strategies, i.e., \nthe TSTC network without using CBP as well as using CBP to fuse \nFig. 6. Disease classification results of the proposed TSTC network.  \nB. Yang et al.",
        "In this paper, a novel multitask deep learning framework called TSTC \nwas proposed for disease and severity classification tasks. It adopted \nSwinT as backbone for generating a multi-branch feature extractor, \nwhich can not only capture long-range semantic interaction information \nbut can extract features targeted for specific tasks. To make features be \nmore discriminative, a feature fusion module was designed using CBP \ntechnique to fuse cross-branch features. In addition, a deep supervision \nmodule was also proposed to supervise hidden layers to generate more \ndiscriminative features in hidden layers. Compared with five widely \nused classification networks (i.e., VGG16, VGG19, ResNet34, ResNet50, \nand SwinT), our TSTC network achieves best accuracy with 99.00% for \ndisease classification and 88.73% for severity classification using the AI \nChallenger 2018 dataset. One of the major contributions of this paper is \nthe development of the multitask deep learning framework, which can",
        "TSTC network provides the best performance among these widely used \nclassification networks. \n4. Discussions \n4.1. Visualization of deep features \nIn order to further explore the feature extraction ability of our pro­\nposed TSTC network, Gradient-weighted Class Activation Map (Selvar­\naju et al., 2017) is used to visualize the features from last layer of the \nfeature extraction module (i.e., feat1 and feat2), as shown in the heat map \nof Fig. 10. It is notable that the deeper the red color, the more attention \nare paid to the features of this part (Liu et al., 2021c). Comparison be­\ntween the disease images and the corresponding heat map suggests the \ndiseased areas are basically consistent with the red parts. The extracted \ndeep feature for severity classification pays extensive attention to the \ndiseased part. As for disease classification, besides the diseased part, the \nextracted deep features also focus on regions surrounding the diseased",
        "CBP is lower than compared to two CBP module is embedded in the \nTSTC network. It is likely to be attributed to that three feature fusion \nmodules cause feature redundancy, which is a burden on the model, thus \nleads to a drop in accuracy. Therefore, CBP2 and CBP3 are thus selected \nto conduct experiments in this paper. \n4.3. Effectiveness of deep supervision \nTo investigate the effect of deep supervision module on multitask \nclassification network, this paper conducted ablation experiments on \nwhether to use the losses of deep supervision module (l3, l4 and l5) or not. \nIn the following experiment, the “Base (w/o l3 + l4 + l5)” denotes the \nbasic model without deep supervision module. The “Base + lk” denotes \nmodel by introducing one loss of deep supervision module into the \n“Base” model, whereas the “Base + lk + lh” denotes model with inte­\ngration any two losses in deep supervision module. Where k, h∈{3,4,5, \nand k ∕= h. The TSTC network is our proposed framework which com­"
      ]
    }
  ]
}