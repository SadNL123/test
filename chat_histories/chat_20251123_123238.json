{
  "id": "20251123_123238",
  "title": "[11-23] 文章的conclusion部分讲了什么",
  "updated_at": "2025-11-23T12:32:50.798384",
  "messages": [
    {
      "role": "user",
      "content": "文章的conclusion部分讲了什么"
    },
    {
      "role": "assistant",
      "content": "根据参考文档片段，文章的结论部分主要包含以下内容：1. **模型性能表现**\n   - TSTC模型在疾病分类和严重程度分类任务中都取得了最佳性能\n   - 在疾病分类中，大多数疾病类别的F1分数都是最高的，其中白粉病和黑麻疹真菌甚至获得了100%的F1分数\n   - 在严重程度分类中，对健康、一般和严重三个等级的F1分数分别达到99.60%、81.95%和87.80%2. **深度监督模块的有效性**\n   - 实验证明深度监督模块能够提高特征提取能力\n   - 当集成l3、l4和l5三个损失函数时，疾病分类和严重程度分类的总体准确率分别提高了1.16%和1.75%3. **特征可视化分析**\n   - 使用梯度加权类激活映射(Grad-CAM)对特征提取模块的最后层特征进行可视化\n   - 热图显示模型能够有效关注到疾病相关的重要特征区域4. **方法优势**\n   - 提出的TSTC网络在多个广泛使用的分类网络中表现最佳\n   - 通过多任务学习、特征融合和深度监督等技术创新，提高了模型的泛化能力和分类准确性这些结果表明作者提出的TSTC网络在植物疾病和严重程度分类任务中具有优越的性能和实际应用价值。",
      "reasoning": "",
      "sources": [
        "cation are shown in Table 2 and Table 3. In disease classification, F1- \nscore of most of the disease categories of our TSTC model is the \nhighest. Powdery mildew and black measles fungus even obtain the F1- \nscore of 100%. In severity classification, our TSTC network achieves the \nhighest F1 of 99.60%, 81.95% and 87.80% for healthy, general and \nserious, respectively, which are 1.32%, 3.23% and 2.46% higher than \nthose obtained by SwinT. These results demonstrates that our proposed \nTSTC network provides the best performance among these widely used \nclassification networks. \n4. Discussions \n4.1. Visualization of deep features \nIn order to further explore the feature extraction ability of our pro­\nposed TSTC network, Gradient-weighted Class Activation Map (Selvar­\naju et al., 2017) is used to visualize the features from last layer of the \nfeature extraction module (i.e., feat1 and feat2), as shown in the heat map \nof Fig. 10. It is notable that the deeper the red color, the more attention \nare paid to the features of this part (Liu et al., 2021c). Comparison be­\ntween the disease images and the corresponding heat map suggests the",
        "disease, and severity are considered as joint categories in most disease classification methods, which will increase \nthe number of categories and decrease the generalization ability of these models. Compared to disease and \nseverity, information on plant species is less important because different species may suffer from the same \ndisease, and such information is usually known to users. Given this, this paper proposed a novel triple-branch \nSwin Transformer classification (TSTC) network for classification of disease and severity simultaneously and \nseparately. The TSTC network consists of a multitask feature extraction module, a feature fusion module and a \ndeep supervision module. Firstly, preliminary features are extracted using a triple-branch network, which is built \nbased on Swin Transformer backbone under the multitask classification strategy (i.e., one for disease classifi­\ncation, one for severity classification and the other for deep supervision). After that, these features are fused \nusing compact bilinear pooling technique to enhance the feature extractor’s learning ability and thus more",
        "categories (leaf spot fungus, scorch, no disease, etc.) and 3 severity \ncategories (healthy, general, and serious) with 30,215 images in all. \nFig. 4 illustrates the statistics of the dataset used in this paper. The \nnumber of images for these diseases varies, with only 65 images for \ntarget spot bacteria, whereas with 3,887 images for YLCL virus. The \nnumber of images for the healthy, general and serious severity are \n6,976, 9,494, and 13,745, respectively. For a better representation, \nFig. 5 shows several samples of disease and severity. As can be seen from \nFig. 5, the rearranged dataset focuses on disease type and the corre­\nsponding severity. \nThe dataset is further randomly split into two parts, i.e., training and \nvalidation datasets, with 80% and 20% of these images, respectively. \nMoreover, in order to reduce overfitting problem caused by the limited \nnumber of images and enhance generalization ability of the model, two \nwidely used data augmentation techniques are utilized to enhance and \ndiversity of the training dataset. The first technique is image resizing, for \nwhich an image can be randomly cropped and resized into the size of",
        "(CBP) was utilized to enhance the discrimination ability of extracting \nfeature of disease. Meanwhile, we designed a deep supervision module \nto enhance the capability of classification. To summarize, the contri­\nbutions of this paper are, \n1) We proposed an end-to-end trainable multitask network for classi­\nfication of disease and severity simultaneously and separately to \nimprove the network’s generalization ability and the application in \nagriculture field.  \n2) We proposed a TST module, which contained sharing part and triple- \nbranch part, to extract features targeted for special task. And \nattributed to the complementarity of different features, we adopted \nCBP for further fusing cross-branch feature information to make \nfeatures more distinctive.  \n3) We explored methods for improving feature extraction ability, then \ndeep supervision module was designed to generate more distinctive \nfeaturesof hidden layers to improve the classification accuracy of \ndisease and severity. \nThe rest of this paper is organized as follows: Section 2 introduces the \nproposed network in detail. Then, Section 3 presents dataset, the results",
        "feature extraction module as feature representations, and utilize these \nrepresentations directly for image classification. These methods are easy \nto use and straightforward. They, however, are not sufficient in the case \nwhen complex diseases occur, and may lead to unsatisfactory classifi­\ncation accuracy (Liang et al., 2019). Studies have shown that fusion of \nfeatures from different branches can help to improve classification \nperformance (Chetouani et al., 2020; Liu and Wu, 2022), which are \nattributed to the complementarity of different features. \nGiven this, a feature fusion module is organized in this paper using a \nwidely used technique, i.e., the CBP technique (Gao et al., 2016). \nDifferent from traditional addition or concatenation of two groups of \nfeatures, CBP first transforms high dimension features from different \nclassification tasks into a low dimensional space using count sketch \nprojection (CSP) function, which can reduce computation and storage \nspace (L´opez-S´anchez et al., 2020). The count sketch projection function \nis adopted because the CSP of the outer product of the input features is",
        "the OA of disease and severity classification is 97.84% and 86.98%, \nrespectively. Incorporated l3 or l4 or l5 into the training process can \nimprove the network performance. For example, the OA can be \nimproved by 0.12% and 0.8% after adding l3 to final loss. It indicates \nthat deep supervision of hidden layers can improve the feature extrac­\ntion ability and thus improves the accuracy of the model. When any two \nlosses of the deep supervision module are used, the OA of both classi­\nfication tasks is improved compared to that without deep supervision. \nHowever, the case is different compared with using only one of the three \nlosses. Take “Base+l3 + l5” model as an example, the accuracy of \nseverity classification is 0.39% higher than “Base+l5” model, whereas \nthe accuracy of disease classification is higher than “Base + l3” model \nand the accuracy of severity classification is lower than “Base + l3” \nmodel. Notably, compared to the “Base” model, the OA of the TSTC with \nl3, l4 and l5 integrated are increased by 1.16% and 1.75% in disease and \nseverity classification, respectively. The great improvement of the TSTC"
      ]
    }
  ]
}