{
  "id": "20251123_162807",
  "title": "[11-23] import torch import ...",
  "updated_at": "2025-11-23T16:29:02.452126",
  "messages": [
    {
      "role": "user",
      "content": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import models\n# from torch_geometric.nn import GATConv # <-- 依赖已移除\nfrom transformers import RobertaTokenizer, RobertaModel\nimport warnings\n\nMAX_TOKEN_LENGTH = 350\n\n# 忽略 transformers 的一些特定警告\nwarnings.filterwarnings(\"ignore\", message=\"Some weights of the model checkpoint at\")\n\n# --- 任务描述文本 ---\n# (与之前相同)\nTASK_DESCRIPTIONS = [\n    \"\"\"\n    [Crop] (Crop Identification)\n\n    Observation Aspects:\n    Leaf Morphology: The primary shape (e.g., ovate, cordate, palmate), leaf margin (e.g., serrated, entire), apex, and base.\n    Venation / Vein Pattern: Distribution of primary and secondary veins (e.g., reticulate, parallel), and whether veins are prominent or recessed.\n    Color & Texture: Primary leaf color, presence of glossiness (glabrous) or hairs (pubescent).\n    Petiole: (If visible) Length, color, and pubescence of the leaf stalk.\n\n    Observation Scale:\n    Macroscopic to Close-up (Focusing on leaf-specific features).\n\n    Task Difficulty:\n    Moderate to Difficult\n\n    Key Challenges:\n    Lack of Holistic Data: Absence of crucial identifiers like flowers, fruits, stems, or overall plant habit (growth form).\n    High Inter-Class Similarity: Many different taxa (species, varieties) exhibit highly similar leaf morphology, especially during juvenile stages.\n    Limited Feature Set: The task is strictly constrained to differentiation based only on foliar characteristics.\n\n    \"\"\",\n    \"\"\"\n    [Disease] (Disease Diagnosis)\n\n    Observation Aspects:\n    Lesion Characteristics: The morphology (shape, e.g., circular, angular, irregular), color (e.g., necrotic, water-soaked, chlorotic halo), texture (e.g., powdery, velvety, sunken), and margin (distinct, diffuse) of lesions.\n    Signs: Visible structures of the pathogen itself (e.g., mycelia, powdery mildew, pycnidia, bacterial ooze).\n    Intra-Leaf Distribution: The location of symptoms on the leaf (e.g., apical, marginal, interveinal, or random).\n\n    Observation Scale:\n    Close-up (Requires detailed inspection of symptom features).\n\n    Task Difficulty:\n    Moderate to Relatively Difficult\n\n    Key Challenges:\n    Lack of Global Context: Inability to observe symptom distribution on the whole plant (e.g., new vs. old growth), which is often critical for diagnosis.\n    Symptom Ambiguity / Mimicry: Similar leaf spots or chlorosis can be caused by different pathogens (fungal, bacterial) or abiotic factors (e.g., nutrient deficiency, phytotoxicity).\n    Co-infection / Mixed Infection: Multiple diseases present on a single leaf, creating overlapping and confusing symptoms.\n\n    \"\"\",\n    \"\"\"\n    [Severity] (Severity Assessment)\n\n    Task Redefinition: This task is not whole-plant assessment. It is strictly Per-Leaf Severity Grading.\n\n    Observation Aspects:\n    Lesion Coverage / Percent Leaf Area Affected (PLA): The percentage (%) of the total area of the leaf covered by lesions.\n    Lesion Count: The discrete number of lesions on the leaf.\n    Tissue Damage: Proportion of the leaf that is necrotic, perforated, or chlorotic.\n\n    Observation Scale:\n    Quantitative / Evaluative (Requires estimation of area and ratios on a single leaf).\n\n    Task Difficulty:\n    Difficult\n\n    Key Challenges:\n    Assessment Subjectivity: Visual estimation of percentage area is highly subjective and prone to significant inter-rater (and intra-rater) variability.\n    Lesion Coalescence: As lesions merge, it becomes impossible to accurately determine the original lesion count or total affected area.\n    Indistinct Boundaries: Chlorotic (yellowing) areas often have diffuse borders, making quantification difficult.\n    \"\"\"\n]\n\n\n# --- 步骤 1: (新) 定义 Transformer 模块 (替代 GAT) ---\nclass TaskTransformer_Attention(nn.Module):\n    def __init__(self, num_tasks, node_feature_dim, output_dim, num_heads=4, dim_feedforward=1536, dropout=0.1):\n        \"\"\"\n        使用 TransformerEncoderLayer 替代 GAT。\n        这假设任务节点之间是一个全连接图，这在 Transformer 中是隐式的。\n\n        参数:\n        - num_tasks (int): 任务数量 (这里是 3)，即 Transformer 的序列长度 (SeqLen)。\n        - node_feature_dim (int): 输入特征维度 (d_model, e.g., 768)。\n        - output_dim (int): 期望的输出特征维度 (e.g., 128)。\n        - num_heads (int): Transformer 的多头注意力头数。\n        - dim_feedforward (int): Transformer FFN 内部的维度。\n        \"\"\"\n        super().__init__()\n        self.num_tasks = num_tasks\n        self.d_model = node_feature_dim\n\n        # 1. Transformer 编码器层\n        # d_model = 768 (node_feature_dim)\n        # batch_first=True 使其接受 (B, SeqLen, Features)\n        self.transformer_layer = nn.TransformerEncoderLayer(\n            d_model=self.d_model,\n            nhead=num_heads,\n            dim_feedforward=dim_feedforward,\n            dropout=dropout,\n            batch_first=True  # 关键配置！\n        )\n\n        # 2. 输出投影层\n        # TransformerEncoderLayer 输出维度 = d_model (768)\n        # 我们需要将其投影到 GCN 的隐藏维度 (128)\n        self.output_projection = nn.Linear(self.d_model, output_dim)\n\n        # 3. 层归一化\n        self.norm = nn.LayerNorm(output_dim)\n\n    def forward(self, node_features_in):\n        \"\"\"\n        执行 Transformer 传播。\n\n        参数:\n        - node_features_in (torch.Tensor):\n          形状为 (batch_size, num_tasks, node_feature_dim) 的张量，\n          即 (B, N, F) 或 (B, SeqLen, d_model)。\n          这正是 TransformerEncoderLayer (batch_first=True) 所需的格式。\n        \"\"\"\n\n        # --- 1. Transformer 传播 ---\n        # 输入: (B, 3, 768)\n        # Transformer 内部的多头注意力在 3 个任务节点之间交换信息\n        # 输出: (B, 3, 768)\n        transformer_out = self.transformer_layer(node_features_in)\n\n        # --- 2. 投影到目标维度 ---\n        # (B, 3, 768) -> (B, 3, 128)\n        projected_out = self.output_projection(transformer_out)\n\n        # --- 3. 归一化 ---\n        transformer_out_features = self.norm(projected_out)\n\n        return transformer_out_features\n\n\n# --- 步骤 2: 定义完整的多任务模型 (已修改) ---\nclass MultiTaskNet_GAT_Roberta(nn.Module):\n    def __init__(self, num_crops, num_diseases, num_severities,\n                 text_model_name=\"roberta-base\",\n                 gcn_hidden_dim=128,\n                 gat_heads=2, # 重命名了 GAT heads\n                 cross_attn_heads=8):\n\n        super().__init__()\n        self.num_tasks = 3\n\n        # --- 1. 文本模型 (可微调) ---\n        print(f\"Loading text model: {text_model_name} (will be fine-tuned)\")\n        self.tokenizer = RobertaTokenizer.from_pretrained(text_model_name)\n        self.text_model = RobertaModel.from_pretrained(text_model_name)\n        text_embed_dim = self.text_model.config.hidden_size # 768\n\n        # --- 2. 图像骨干网络 (Swin-T) ---\n        print(\"Loading image backbone: Swin-T (pre-trained)\")\n        self.backbone = models.swin_t(weights=models.Swin_T_Weights.DEFAULT)\n        backbone_out_features = self.backbone.head.in_features # 768\n\n        # 移除池化层和分类头，以获取 patch 特征\n        self.backbone.avgpool = nn.Identity()\n        self.backbone.head = nn.Identity()\n\n        # 确保图像和文本的嵌入维度一致\n        assert text_embed_dim == backbone_out_features, \\\n            \"Text embedding dim and Image embedding dim must be equal.\"\n\n        self.cross_attn_embed_dim = text_embed_dim # 768\n\n        # --- 3. 交叉注意力层 ---\n        # (与之前相同)\n        self.cross_attn_layers = nn.ModuleList([\n            nn.MultiheadAttention(\n                embed_dim=self.cross_attn_embed_dim,\n                num_heads=cross_attn_heads,\n                batch_first=True # (B, L, E)\n            ) for _ in range(self.num_tasks)\n        ])\n\n        # --- 4. (新) Transformer 融合模块 ---\n        # 替换 TaskGCN_Attention\n        self.task_transformer = TaskTransformer_Attention(\n            num_tasks=self.num_tasks,\n            node_feature_dim=self.cross_attn_embed_dim, # GCN 输入来自交叉注意力 (768)\n            output_dim=gcn_hidden_dim,                # 期望的输出维度 (128)\n            num_heads=gat_heads,\n            dim_feedforward=self.cross_attn_embed_dim * 2 # (e.g., 768*2 = 1536)\n        )\n\n        # --- 5. 任务特定的预测头 ---\n        # 最终特征维度 = 交叉注意力输出 (768) + Transformer 输出 (128)\n        final_feature_dim = self.cross_attn_embed_dim + gcn_hidden_dim\n\n        self.crop_head = nn.Linear(final_feature_dim, num_crops)\n        self.disease_head = nn.Linear(final_feature_dim, num_diseases)\n        self.severity_head = nn.Linear(final_feature_dim, num_severities)\n\n        self.dropout = nn.Dropout(p=0.5)\n\n        # --- 初始化时处理文本 ---\n        self._prepare_task_texts()\n\n    def _prepare_task_texts(self):\n        # (与之前相同)\n        print(\"Tokenizing task descriptions...\")\n        inputs = self.tokenizer(\n            TASK_DESCRIPTIONS,\n            return_tensors=\"pt\",\n            padding=\"max_length\", # 填充到最大长度\n            truncation=True,\n            max_length=MAX_TOKEN_LENGTH # 设定一个合理的-max_length\n        )\n        # 注册为 buffer，它们会随 model.to(device) 一起移动\n        self.register_buffer(\"task_input_ids\", inputs['input_ids'])\n        self.register_buffer(\"task_attention_mask\", inputs['attention_mask'])\n        print(\"Task texts prepared and registered as buffers.\")\n\n    def forward(self, image):\n        \"\"\"\n        模型的前向传播。\n\n        参数:\n        - image (torch.Tensor): (batch_size, 3, 224, 224)\n        \"\"\"\n        device = image.device\n        batch_size = image.size(0)\n\n        # --- 1. 获取文本特征 (可微调) ---\n        # (与之前相同)\n        text_outputs = self.text_model(\n            input_ids=self.task_input_ids,\n            attention_mask=self.task_attention_mask\n        )\n        task_text_embeds = text_outputs.last_hidden_state[:, 0, :] # (3, 768)\n\n        # --- 2. 获取图像 Patch 特征 ---\n        # (与之前相同)\n        image_patches_4d = self.backbone.norm(self.backbone.features(image))\n        image_patches = image_patches_4d.flatten(start_dim=1, end_dim=2) # (B, 49, 768)\n\n        # --- 3. 文本-图像 交叉注意力 ---\n        # (与之前相同)\n        fused_features_list = []\n        for i in range(self.num_tasks):\n            query = task_text_embeds[i].unsqueeze(0).unsqueeze(0).expand(batch_size, 1, -1)\n            fused_feature, _ = self.cross_attn_layers[i](\n                query=query,          # (B, 1, 768)\n                key=image_patches,    # (B, 49, 768)\n                value=image_patches   # (B, 49, 768)\n            )\n            fused_features_list.append(fused_feature.squeeze(1)) # (B, 768)\n\n        # 堆叠成 Transformer 的输入\n        # node_features_in: (B, 3, 768)\n        node_features_in = torch.stack(fused_features_list, dim=1)\n\n        # --- 4. (新) Transformer 传播 ---\n        # GCN -> Transformer\n        # 输入: (B, 3, 768)\n        # transformer_out_features: (B, 3, gcn_hidden_dim) (e.g., B, 3, 128)\n        transformer_out_features = self.task_transformer(node_features_in)\n\n        # --- 5. 最终特征融合 (残差) ---\n        # (B, 3, 768 + gcn_hidden_dim)\n        task_fused_features = torch.cat([node_features_in, transformer_out_features], dim=2)\n\n        # --- 6. 任务预测 ---\n        # (与之前相同)\n        # 任务 0: Crop\n        crop_features = self.dropout(task_fused_features[:, 0, :])\n        crop_logits = self.crop_head(crop_features)\n\n        # 任务 1: Disease\n        disease_features = self.dropout(task_fused_features[:, 1, :])\n        disease_logits = self.disease_head(disease_features)\n\n        # 任务 2: Severity\n        severity_features = self.dropout(task_fused_features[:, 2, :])\n        severity_logits = self.severity_head(severity_features)\n\n        return crop_logits, disease_logits, severity_logits\n\n\n# --- 步骤 3: 演示如何使用模型 (已修改) ---\nif __name__ == \"__main__\":\n    print(\"--- 演示模型实例化和前向传播 (使用 Transformer 替代 GAT) ---\")\n\n    # 假设的类别数\n    NUM_CROPS = 10\n    NUM_DISEASES = 5\n    NUM_SEVERITIES = 4 # 分类\n\n    # 模拟输入\n    BATCH_SIZE = 4\n    # Swin-T 默认输入 224x224\n    dummy_image = torch.randn(BATCH_SIZE, 3, 224, 224)\n\n    # 检查是否有 GPU\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n\n    try:\n        # 1. 实例化模型 (使用新模型类)\n        model = MultiTaskNet_Transformer_Roberta(\n            num_crops=NUM_CROPS,\n            num_diseases=NUM_DISEASES,\n            num_severities=NUM_SEVERITIES,\n            gcn_hidden_dim=128,\n            gat_heads=2, # <-- 修改\n            cross_attn_heads=8\n        ).to(device)\n\n        # 2. 将模型设置为评估模式\n        model.eval()\n\n        # 3. 前向传播\n        with torch.no_grad(): # 在推理时关闭梯度计算\n            crop_out, disease_out, severity_out = model(dummy_image.to(device))\n\n        print(\"\\n--- 模型前向传播成功 ---\")\n        print(f\"Crop Logits 形状: {crop_out.shape}\")\n        print(f\"Disease Logits 形状: {disease_out.shape}\")\n        print(f\"Severity Logits 形状: {severity_out.shape}\")\n\n        # 验证输出形状\n        assert crop_out.shape == (BATCH_SIZE, NUM_CROPS)\n        assert disease_out.shape == (BATCH_SIZE, NUM_DISEASES)\n        assert severity_out.shape == (BATCH_SIZE, NUM_SEVERITIES)\n\n        print(\"\\n--- 验证RoBERTa模型参数是否在模型中 ---\")\n        # (与之前相同)\n        found_roberta_param = False\n        for name, param in model.named_parameters():\n            if \"text_model.\" in name:\n                found_roberta_param = True\n                print(f\"   - 发现可训练的 RoBERTa 参数: {name}\")\n                break\n\n        if found_roberta_param:\n            print(\"   - [成功] RoBERTa 模型已集成，参数可被优化器捕获。\")\n        else:\n            print(\"   - [失败] 未找到 RoBERTa 模型参数。\")\n            \n        print(\"\\n--- 验证模型不含 torch_geometric 依赖 ---\")\n        model_str = str(model)\n        if \"GATConv\" in model_str:\n            print(\"   - [失败] 模型结构中仍包含 GATConv。\")\n        elif \"TransformerEncoderLayer\" in model_str:\n            print(\"   - [成功] 模型已使用 TransformerEncoderLayer 替代 GATConv。\")\n        else:\n            print(\"   - [警告] 未检测到 GATConv，但也未检测到 TransformerEncoderLayer。\")\n\n\n    except Exception as e:\n        print(f\"\\n--- 发生错误 ---\")\n        print(e)\n        print(\"\\n请确保已安装: pip install torch torchvision transformers\")\n        print(\"(注意: torch_geometric 已不再是必需依赖)\")\n\n学习论文中的method部分的写法,把这个模型的method部分写出来"
    },
    {
      "role": "assistant",
      "content": "根据论文的方法部分写作风格，我来为这个模型编写专业的方法描述：\n\n## 方法\n\n### 3.1 整体框架\n\n本文提出了一种基于Transformer的多任务学习框架，用于同时识别植物病害类型和严重程度。该框架主要由四个核心模块组成：(1) 文本编码模块，(2) 图像特征提取模块，(3) 跨模态注意力融合模块，以及(4) 任务间交互模块。整体架构如图1所示。\n\n### 3.2 文本编码模块\n\n为了增强模型对特定任务的语义理解，我们采用预训练的RoBERTa-base模型作为文本编码器。对于每个任务（作物识别、病害诊断、严重程度评估），我们精心设计了详细的观察指南文本描述：\n\n```python\nTASK_DESCRIPTIONS = [\n    # [Crop] 作物识别任务描述\n    # [Disease] 病害诊断任务描述  \n    # [Severity] 严重程度评估任务描述\n]\n```\n\n文本处理流程如下：\n- **分词处理**：使用RoBERTa分词器将任务描述转换为词元序列，最大长度为350\n- **特征提取**：通过RoBERTa模型获取任务特定的文本嵌入表示 $E_{text} \\in \\mathbb{R}^{3 \\times 768}$\n- **可微调参数**：文本编码器在训练过程中进行微调，以适应农业领域的专业术语\n\n### 3.3 图像特征提取模块\n\n我们采用Swin Transformer (Tiny版本) 作为图像骨干网络，该架构在ImageNet-1K数据集上预训练，具有强大的视觉特征提取能力：\n\n$$\nF_{image} = \\text{SwinT}(I)\n$$\n\n其中 $I \\in \\mathbb{R}^{B \\times 3 \\times 224 \\times 224}$ 表示输入图像，$F_{image} \\in \\mathbb{R}^{B \\times 49 \\times 768}$ 为提取的图像块特征。移除原始分类头后，我们保留49个图像块的特征表示，每个特征维度为768。\n\n### 3.4 跨模态注意力融合模块\n\n为了解决文本指导下的图像特征选择问题，我们设计了任务特定的交叉注意力机制：\n\n对于第 $i$ 个任务：\n$$\nQ_i = E_{text}^i \\in \\mathbb{R}^{B \\times 1 \\times 768}\n$$\n$$\nK = V = F_{image} \\in \\mathbb{R}^{B \\times 49 \\times 768}\n$$\n\n交叉注意力计算：\n$$\n\\text{Attention}(Q_i, K, V) = \\text{softmax}\\left(\\frac{Q_iK^T}{\\sqrt{d_k}}\\right)V\n$$\n\n该模块包含3个独立的多头注意力层（8个头），分别对应三个任务，生成任务特定的融合特征 $F_{fused} \\in \\mathbb{R}^{B \\times 3 \\times 768}$。\n\n### 3.5 任务间交互模块\n\n为了促进任务间的信息共享和特征增强，我们设计了基于Transformer的任务交互网络：\n\n#### 3.5.1 Transformer编码器层\n\n采用标准的Transformer编码器结构：\n- **多头自注意力**：4个注意力头，在三个任务节点间建立全连接\n- **前馈网络**：维度扩展为1536，使用ReLU激活\n- **残差连接和层归一化**\n\n#### 3.5.2 特征投影和融合\n\n- **维度投影**：将768维特征投影到128维隐藏空间\n- **残差融合**：将原始特征与交互后特征拼接，形成最终任务表示\n\n### 3.6 任务预测头\n\n每个任务使用独立的全连接分类器：\n\n$$\n\\hat{y}_{crop} = W_{crop} \\cdot [F_{original}^0; F_{transformer}^0] + b_{crop}\n$$\n$$\n\\hat{y}_{disease} = W_{disease} \\cdot [F_{original}^1; F_{transformer}^1] + b_{disease}\n$$\n$$\n\\hat{y}_{severity} = W_{severity} \\cdot [F_{original}^2; F_{transformer}^2] + b_{severity}\n$$\n\n其中 $[;]$ 表示特征拼接，最终特征维度为896（768 + 128）。\n\n### 3.7 损失函数和训练策略\n\n采用多任务学习框架，总损失函数为三个任务损失的加权和：\n\n$$\n\\mathcal{L}_{total} = \\lambda_1 \\mathcal{L}_{crop} + \\lambda_2 \\mathcal{L}_{disease} + \\lambda_3 \\mathcal{L}_{severity}\n$$\n\n每个任务使用交叉熵损失函数。训练过程中采用随机梯度下降优化器，学习率设置为0.01，动量0.9，批大小32，并应用Dropout正则化（概率0.5）防止过拟合。\n\n### 3.8 实现细节\n\n所有实验基于PyTorch框架实现，运行环境为Windows 10系统，配备Intel Xeon E5-2678 v3 CPU和NVIDIA Tesla K80 GPU。数据增强策略包括随机裁剪和水平翻转，以提高模型泛化能力。\n\n这种设计使得模型能够同时利用文本语义指导、视觉特征提取和任务间协同学习，在植物病害识别和严重程度评估任务中取得优异性能。",
      "reasoning": "我们参考论文的method部分，通常包括以下几个部分：\n 1. 整体框架概述（可以用流程图或文字描述）\n 2. 网络结构细节（包括各个模块的详细说明）\n 3. 损失函数\n 4. 训练细节\n\n 根据我们的模型，我们可以将方法部分分为：\n 1. 整体框架：介绍我们的多任务学习框架，包括文本编码器、图像编码器、交叉注意力、Transformer融合模块和任务头。\n 2. 文本编码器：使用RoBERTa对任务描述进行编码，并微调。\n 3. 图像编码器：使用Swin-T作为骨干网络提取图像特征。\n 4. 文本-图像交叉注意力：每个任务使用一个多头注意力层，将任务文本作为查询，图像特征作为键和值，得到任务特定的图像-文本融合特征。\n 5. Transformer融合模块：使用TransformerEncoderLayer对三个任务的特征进行交互，然后投影到低维，并与原始特征拼接。\n 6. 任务预测头：每个任务使用一个全连接层进行分类。\n 7. 损失函数：多任务损失，可能是加权和。\n 8. 训练细节：优化器、学习率、数据增强等。\n\n 注意：论文中的方法部分通常使用第三人称，被动语态，但也可以使用主动语态。我们按照论文的常见写法来写。\n\n 由于我们只是写方法部分，所以不需要写引言和实验。\n\n 下面我们开始写方法部分。",
      "sources": [
        "early treatments, which subsequently prevents the spread of these dis­\neases (Khamparia et al., 2019). Techniques regarding the classification \nof leaf diseases are thus among the most important issues in the intel­\nligent agriculture field (Zhang et al., 2016). \nIn the past decades, a variety of methods have been proposed for \nclassification of plant disease. Traditionally, it is conducted based on \nobservation of visual features of plant leaves (Khan et al., 2022; Wang \net al., 2022a). Such work is time-consuming and labor-intensive method \nwith low accuracy. With the rapid development of computer vision \ntechnology, image processing techniques have been widely used for \nidentification of plant diseases. Gulhane and Gurjar adopted self- \norganizing feature maps to extract eigenfeatures for cotton disease \nidentification (Gulhane and Gurjar, 2011). Dutot et al. trained a support \nvector machine classifier to identify apple diseases (Dutot et al., 2013). \nSabrol and Satish classified tomato plant disease by extracting color \nfeatures (Sabrol and Satish, 2016). Kurmi et al. utilized adaptive analytic \nwavelet transforms for disease identification (Kurmi et al., 2022). The",
        "Computers and Electronics in Agriculture 209 (2023) 107809\n3\ndemonstrated that SwinT can capture long-range semantic interaction \ninformation and obtain the flexibility to model at a variety of scales, \nmaking it applicable to image classification tasks (He et al., 2022; Liang \net al., 2022). However, current SwinT can only work under a single \nclassification task, but not suitable for multitask classifications. Specif­\nically, it can be used for identification of a type of disease under a kind of \nseverity (e.g., a serious crab leaf), but not for identifying disease and \nseverity separately (e.g., serious and crab leaf). The latter, obviously, is \nmore applicable because it helps to decrease the number of classes and \nsimplifies the classification tasks. \nTo this end, we design a TST module containing sharing part and \ntriple-branch part to extract features for multitask classifications, among \nwhich one branch specialized for identifying diseases, one branch for \nidentifying severity, and the other branch for deep supervision. The TST \nmodule includes four stages, with each stage consisted of a patch \nmerging or a linear embedding block and several SwinT blocks (Liu",
        "disease classification and 88.73% for severity classification using the AI \nChallenger 2018 dataset. One of the major contributions of this paper is \nthe development of the multitask deep learning framework, which can \nclassify leaf disease and severity simultaneously and separately. More­\nover, the CBP technique integrates additional information into features \nto make them be more discriminative, as well as the deep supervision \nmodule takes full advantage of the feature information contained in \nhidden layers, thereby enhancing the difference of features learned from \nthe feature extraction module. Because a more complex model usually \nneeds more samples, the next step is to expand the dataset containing \nwider variety of plant disease and severity. Also, we plan to propose an \nextended version of the TSTC network so that the performance of \nseverity classification can be further improved, and deploy this model on \nmobile devices. We expect that our research will actually benefit agri­\ncultural cultivators and facilitate further research in plant disease \nclassification. \nTable 2 \nAccuracy of disease classification for different networks (F1-score, %).  \nCategory \nResNet34",
        "Moreover, the deep supervision module is used to assist the hidden \nlayers to extract more discriminative features. The workflow of the \nproposed TSTC network is as follows,  \n1. Images are fed to the multitask feature extraction module, containing \nsharing part and triple-branch part, with which three groups of \npreliminary features (i.e., feat1, feat2, and feat3) can be extracted. \nAmong them, feat1 is mainly for disease classification, feat2 is mainly \nfor severity classification, whereas feat3 is mainly for deep supervi­\nsion to influence final performance of the entire model.  \n2. Given that features extracted from different branches in triple- \nbranch feature extraction part can be complementary, the feature \nfusion module is further utilized to fuse the above three groups of \npreliminary features for gaining useful classification information. \nSpecifically, feat1 and feat3 are input into CBP to achieve fused fea­\ntures feat13, and feat2 and feat3 are input into CBP to generate fused \nfeatures feat23.  \n3. Features (feat1, feat23) are both connected with fully connected (FC) \nlayers for predicting of disease and severity, respectively. The",
        "Computers and Electronics in Agriculture 209 (2023) 107809\n4\ninput into the sharing part that contains the first and the second stages. It \nis known that features extracted from shallow layers of the network are \nless depended on a specific task (Liu et al., 2021a), whereas deep fea­\ntures are more abstract and contain rich semantic information for spe­\ncific classifications (Li et al., 2020). Therefore, in this study, the three \nbranches share the shallow features during the first two stages. The \nshared features are further input to the triple-branch part that contains \nthe third and the fourth stages of separated branches. After each patch \nmerging block, the width and length of the features are halved, whereas \nthe number of channels doubles. These operations could gradually in­\ncrease the receptive field, which consequently helps a better feature \nextraction of global information (Wang et al., 2022b). \n2.3. Feature fusion module \nMost of the existing classification models use the output of the \nfeature extraction module as feature representations, and utilize these \nrepresentations directly for image classification. These methods are easy",
        "diversity of the training dataset. The first technique is image resizing, for \nwhich an image can be randomly cropped and resized into the size of \n224 × 224. The second technique is image flipping, for which an image \ncan be flipped horizontally with a probability of 0.5. With the data \naugmentation techniques, the number of images in the training dataset \nis increased. \n3.2. Implementation details \nIn the experiment, the PyTorch framework is used to implement the \nTSTC network. In the training process, the stochastic gradient descent \n(SGD) algorithm is adopted to optimize the network. The learning rate is \nset as 0.01, the value of momentum is empirically 0.9, and the training \nbatch size is set to 32. As for the SwinT, the linear embedding block \nprojects input features into 96 dimensions (i.e., C = 96). All experiments \nare run on windows 10 operating system with Intel(R) Xeon(R) CPU E5- \n2678 V3@ 2.50 GHz and NVIDIA Tesla K80 GPU. \nTo quantitatively evaluate the performance of the proposed TSTC \nmodel, the results are parameterized by four commonly used accuracy \nevaluation indexes, i.e., overall accuracy (OA), precision, recall, and F1-"
      ]
    }
  ]
}