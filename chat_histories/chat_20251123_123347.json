{
  "id": "20251123_123347",
  "title": "[11-23] 这个论文的method部分和discus...",
  "updated_at": "2025-11-23T12:57:48.354061",
  "messages": [
    {
      "role": "user",
      "content": "这个论文的method部分和discussion部分是怎么组织的"
    },
    {
      "role": "assistant",
      "content": "根据提供的参考文档片段，这篇论文的Method（方法）部分和Discussion（讨论）部分的组织结构如下：## Method（方法）部分组织：**2.1. Overview（概述）**\n- 首先介绍了提出的TSTC网络整体架构（如图1所示）\n- 网络包含三个主要模块：\n  1. **多任务特征提取模块** - 包含TST模块（共享部分和三分支部分）\n  2. **特征融合模块** - 使用CBP技术融合初步特征\n  3. **深度监督模块** - 辅助隐藏层提取更具判别性的特征**具体技术细节：**\n- 详细描述了TST模块的设计：共享部分和三分支部分\n- 解释了特征融合模块采用CBP技术的原因和优势\n- 说明了深度监督模块的作用机制## Discussion（讨论）部分组织：从文档片段可以看出，Discussion部分主要包含：**3.4. Results of severity classification（严重程度分类结果）**\n- 展示TSTC网络在严重程度分类方面的性能\n- 使用定量指标（精确率、召回率、F1分数）进行评估\n- 分析不同严重程度类别的分类效果\n- 通过混淆矩阵进一步验证模型性能**可视化分析和实验讨论**（在Section 4中）\n- 对实验结果进行深入分析\n- 讨论网络的优势和局限性\n- 与其他广泛使用的网络进行比较论文采用标准的学术论文结构：先详细介绍方法设计，然后通过实验结果和可视化分析来讨论方法的有效性和性能表现。",
      "reasoning": "",
      "sources": [
        "disease and severity. \nThe rest of this paper is organized as follows: Section 2 introduces the \nproposed network in detail. Then, Section 3 presents dataset, the results \nusing quantitative metrics and confusion matrix, and the comparison \nwith widely used networks. Section 4 provides visualization analysis and \ndiscusses the experiment performed. Finally, Section 5 concludes this \npaper. \n2. Method \n2.1. Overview \nThe architecture of the proposed TSTC network is illustrated in \nFig. 1. It consists of three modules: a multitask feature extraction \nmodule, a feature fusion module, and a deep supervision module. In the \nmultitask feature extraction module, we design a TST module consisting \nof sharing part and triple-branch part to extract preliminary features \nspecialized for disease and severity classification tasks. The feature \nfusion module further combines these preliminary complementary fea­\ntures using CBP technique so that these features can be enhanced. \nMoreover, the deep supervision module is used to assist the hidden \nlayers to extract more discriminative features. The workflow of the \nproposed TSTC network is as follows,",
        "model, the results are parameterized by four commonly used accuracy \nevaluation indexes, i.e., overall accuracy (OA), precision, recall, and F1- \nscore (Liu et al., 2022). The definitions of OA, precision, recall, and F1- \nscore are shown in Eq. (9–12). In this paper, OA is an overall indicator \nthat used to quantitatively evaluate the overall performance of the \nnetwork. Precision, recall, and F1-score are used to evaluate accuracy of \neach category (i.e., disease type or severity). In addition, confusion \nmatrix is used to further validate the performance of the model for each \ncategory. \nOA =\nTP + TN\nTP + FP + TN + FN\n(9)  \nPrecision =\nTP\nTP + FP\n(10)  \nRecall =\nTP\nTP + FN\n(11)  \nF1 −score = 2*precision*recall\nprecision + recall\n(12)  \nwhere TP is True Positive, TN is True Negative, FP is False Positive, and \nB. Yang et al.",
        "feature extraction module as feature representations, and utilize these \nrepresentations directly for image classification. These methods are easy \nto use and straightforward. They, however, are not sufficient in the case \nwhen complex diseases occur, and may lead to unsatisfactory classifi­\ncation accuracy (Liang et al., 2019). Studies have shown that fusion of \nfeatures from different branches can help to improve classification \nperformance (Chetouani et al., 2020; Liu and Wu, 2022), which are \nattributed to the complementarity of different features. \nGiven this, a feature fusion module is organized in this paper using a \nwidely used technique, i.e., the CBP technique (Gao et al., 2016). \nDifferent from traditional addition or concatenation of two groups of \nfeatures, CBP first transforms high dimension features from different \nclassification tasks into a low dimensional space using count sketch \nprojection (CSP) function, which can reduce computation and storage \nspace (L´opez-S´anchez et al., 2020). The count sketch projection function \nis adopted because the CSP of the outer product of the input features is",
        "(CBP) was utilized to enhance the discrimination ability of extracting \nfeature of disease. Meanwhile, we designed a deep supervision module \nto enhance the capability of classification. To summarize, the contri­\nbutions of this paper are, \n1) We proposed an end-to-end trainable multitask network for classi­\nfication of disease and severity simultaneously and separately to \nimprove the network’s generalization ability and the application in \nagriculture field.  \n2) We proposed a TST module, which contained sharing part and triple- \nbranch part, to extract features targeted for special task. And \nattributed to the complementarity of different features, we adopted \nCBP for further fusing cross-branch feature information to make \nfeatures more distinctive.  \n3) We explored methods for improving feature extraction ability, then \ndeep supervision module was designed to generate more distinctive \nfeaturesof hidden layers to improve the classification accuracy of \ndisease and severity. \nThe rest of this paper is organized as follows: Section 2 introduces the \nproposed network in detail. Then, Section 3 presents dataset, the results",
        "disease, and severity are considered as joint categories in most disease classification methods, which will increase \nthe number of categories and decrease the generalization ability of these models. Compared to disease and \nseverity, information on plant species is less important because different species may suffer from the same \ndisease, and such information is usually known to users. Given this, this paper proposed a novel triple-branch \nSwin Transformer classification (TSTC) network for classification of disease and severity simultaneously and \nseparately. The TSTC network consists of a multitask feature extraction module, a feature fusion module and a \ndeep supervision module. Firstly, preliminary features are extracted using a triple-branch network, which is built \nbased on Swin Transformer backbone under the multitask classification strategy (i.e., one for disease classifi­\ncation, one for severity classification and the other for deep supervision). After that, these features are fused \nusing compact bilinear pooling technique to enhance the feature extractor’s learning ability and thus more",
        "(only 65 images for this disease) compared to other diseases, and thus \nleads to a relatively low accuracy. As shown in Fig. 7, the confusion \nmatrix is used to demonstrate the performance of the TSTC network. The \ndiagonal of the matrix represents the number of correct disease pre­\ndictions, whereas the rest of the matrix represent misclassification. It can \nbe found that most of the leaves are predicted correctly, and only a very \nsmall number of leaves are misclassified. For example, 131 images of \nleaf mold fungus are predicted correctly and only 1 image are predicted \nincorrectly by the network. It is thus clear that the TSTC network works \nwell for classification of leaf diseases. \n3.4. Results of severity classification \nAs for severity classification, the performance of the TSTC network is \nshown in Fig. 8. It can be seen that the classification results of the three \ncategories are above 80%, and three evaluation indexes of healthy is the \nhighest. For the healthy category in severity category, precision, recall \nand F1-score were 99.71%, 99.50% and 99.60%, respectively. However, \nthe classification results of serious and general in disease severity are"
      ]
    },
    {
      "role": "user",
      "content": "文章的标题是"
    },
    {
      "role": "assistant",
      "content": "很抱歉，根据您提供的对话历史，我无法确定您所指的具体论文标题。目前我看到的上下文信息主要涉及论文的**方法**和**讨论**部分的组织结构，但没有包含该论文的标题信息。为了帮助您找到准确的标题，您可以：1.  **提供完整的论文文档**：如果您有该论文的PDF或文本文件，请将其提供给我。\n2.  **告知论文的研究领域或关键词**：例如，它是否是关于“基于TSTC网络的严重程度分类”或其他领域的论文。一旦您提供了更多信息，我将很乐意为您查找或确认论文的标题。",
      "reasoning": "",
      "sources": []
    },
    {
      "role": "user",
      "content": "就是这个文章的标题"
    },
    {
      "role": "assistant",
      "content": "很抱歉，从我目前能看到的【参考文档片段】中，**无法直接找到这篇论文的完整标题**。文档片段是论文正文的一部分，主要包含引言、方法、实验结果和讨论等内容，但通常不包含位于最开头的完整标题。不过，我可以根据文档内容，为您推断出这篇论文**可能**的标题形式：### 推断出的可能标题要素根据文档内容，这篇论文的核心工作是：*   **主要任务**：同时但分别地识别**植物病害**和其**严重程度**。\n*   **核心方法**：提出了一个名为 **TSTC** 的网络。\n*   **技术基础**：基于 **Swin Transformer** 模型。因此，一个非常接近的标题可能类似于：> **基于三分支SwinT分类网络（TSTC）的植物病害与严重程度多任务识别**或者更简洁的学术标题形式：> **TSTC: 一个用于植物病害与严重程度分类的三分支Swin Transformer网络**### 如何找到确切标题要获得确切的标题，您可以通过以下方式：1.  **查看文档最开头部分**：论文的标题总是出现在作者姓名和摘要之前。\n2.  **根据文献引用信息查找**：文档片段中包含了期刊信息 `Computers and Electronics in Agriculture` 和 DOI 号 `https://doi.org/10.1016/j.compag.2023.107809`。您可以使用这个DOI号在互联网上搜索，就能直接找到这篇论文的官方页面和完整标题。如果您能提供文档的开头部分，我可以立即帮您确认准确的标题。",
      "reasoning": "",
      "sources": [
        "the species of plant is usually known for the users or planters (Wang \net al., 2021). Therefore, accurate identification of plant disease and \nseverity is more important, and a new strategy for identifying disease \nand severity is needed, which can improve the versatility and general­\nizability in the task of plant disease identification. However, as far as is \nknow, we are unaware of any work that conducts multitask for identi­\nfying disease and severity simultaneously and separately. \nTo tackle this problem, this paper built a triple-branch SwinT clas­\nsification (TSTC) network to achieve multitask, in order to separate \nclassification of disease and severity for the flexibility of plant disease \nclassification. SwinT can pay attention to the relevant information of \nother adjacent windows, has excellent performance in feature extrac­\ntion. Therefore, we designed a triple-branch SwinT (TST) module based \non SwinT for feature extraction, and then compact bilinear pooling \n(CBP) was utilized to enhance the discrimination ability of extracting \nfeature of disease. Meanwhile, we designed a deep supervision module",
        "categories (leaf spot fungus, scorch, no disease, etc.) and 3 severity \ncategories (healthy, general, and serious) with 30,215 images in all. \nFig. 4 illustrates the statistics of the dataset used in this paper. The \nnumber of images for these diseases varies, with only 65 images for \ntarget spot bacteria, whereas with 3,887 images for YLCL virus. The \nnumber of images for the healthy, general and serious severity are \n6,976, 9,494, and 13,745, respectively. For a better representation, \nFig. 5 shows several samples of disease and severity. As can be seen from \nFig. 5, the rearranged dataset focuses on disease type and the corre­\nsponding severity. \nThe dataset is further randomly split into two parts, i.e., training and \nvalidation datasets, with 80% and 20% of these images, respectively. \nMoreover, in order to reduce overfitting problem caused by the limited \nnumber of images and enhance generalization ability of the model, two \nwidely used data augmentation techniques are utilized to enhance and \ndiversity of the training dataset. The first technique is image resizing, for \nwhich an image can be randomly cropped and resized into the size of",
        "are paid to the features of this part (Liu et al., 2021c). Comparison be­\ntween the disease images and the corresponding heat map suggests the \ndiseased areas are basically consistent with the red parts. The extracted \ndeep feature for severity classification pays extensive attention to the \ndiseased part. As for disease classification, besides the diseased part, the \nextracted deep features also focus on regions surrounding the diseased \nareas. In addition, for the four healthy leaves in Fig, 7 (a), (d), (g), and \n(j), the features for two classification tasks are almost focused on the \nsame part, i.e., the whole leaf. It is thus clear that discriminative features \ncan be extracted via the feature extraction module. \n4.2. Effectiveness of feature fusion \nTo explore the effectiveness of the feature fusion module, we per­\nformed ablation experiments using several feature fusion strategies, i.e., \nthe TSTC network without using CBP as well as using CBP to fuse \nFig. 6. Disease classification results of the proposed TSTC network.  \nB. Yang et al.",
        "field of plant diseases and severity classification (Gao et al., 2021). It \nconsists of 31,718 disease images, which are divided into 61 categories \nby species, disease type or severity, such as potato early bling fungus \ngeneral, and tomato early bling fungus general, et al. It is notable that \nthis paper aims to identify plant disease and the corresponding severity \naccurately, the leaf species information is thus less important because \ndifferent species may suffer from the same disease and considering more \ncategories will decrease the generalization ability of the proposed \nmodel. Given this, this paper ignores plant species category and rear­\nrange the AI Challenger 2018 dataset into two categories, i.e., disease \nand severity. All healthy leaves, despite from different species, are \nclassified to the “no disease” type. Three diseases without severity in­\nformation (i.e., apple frogeye spot, corn maize dwarf mosaic virus, to­\nmato TOMV) in the dataset are excluded, remaining 20 disease \ncategories (leaf spot fungus, scorch, no disease, etc.) and 3 severity \ncategories (healthy, general, and serious) with 30,215 images in all.",
        "features (Sabrol and Satish, 2016). Kurmi et al. utilized adaptive analytic \nwavelet transforms for disease identification (Kurmi et al., 2022). The \n* Corresponding author. \nE-mail address: mkslch2013@163.com (C. Li).  \nContents lists available at ScienceDirect \nComputers and Electronics in Agriculture \njournal homepage: www.elsevier.com/locate/compag \nhttps://doi.org/10.1016/j.compag.2023.107809 \nReceived 20 September 2022; Received in revised form 13 March 2023; Accepted 25 March 2023",
        "(only 65 images for this disease) compared to other diseases, and thus \nleads to a relatively low accuracy. As shown in Fig. 7, the confusion \nmatrix is used to demonstrate the performance of the TSTC network. The \ndiagonal of the matrix represents the number of correct disease pre­\ndictions, whereas the rest of the matrix represent misclassification. It can \nbe found that most of the leaves are predicted correctly, and only a very \nsmall number of leaves are misclassified. For example, 131 images of \nleaf mold fungus are predicted correctly and only 1 image are predicted \nincorrectly by the network. It is thus clear that the TSTC network works \nwell for classification of leaf diseases. \n3.4. Results of severity classification \nAs for severity classification, the performance of the TSTC network is \nshown in Fig. 8. It can be seen that the classification results of the three \ncategories are above 80%, and three evaluation indexes of healthy is the \nhighest. For the healthy category in severity category, precision, recall \nand F1-score were 99.71%, 99.50% and 99.60%, respectively. However, \nthe classification results of serious and general in disease severity are"
      ]
    }
  ]
}